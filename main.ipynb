{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     [17:28:00] üîç Starting the research task for 'What are the best ways to use a loss functions when fine tuning an LLM model?'...\n",
      "INFO:     [17:28:00] Default Agent\n",
      "INFO:     [17:28:00] üåê Browsing the web to learn more about the task: What are the best ways to use a loss functions when fine tuning an LLM model?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error in reading JSON, attempting to repair JSON\n",
      "Error decoding JSON: Invalid \\escape: line 3 column 7 (char 46)\n",
      "No JSON found in the string. Falling back to Default Agent.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     [17:28:04] ü§î Planning the research strategy and subtasks (this may take a minute)...\n",
      "INFO:     [17:28:05] üóÇÔ∏è I will conduct my research based on the following queries: ['best practices for loss function selection in LLM fine-tuning', 'LLM fine-tuning overfitting prevention methods with loss functions', 'parameter efficient fine-tuning and loss functions for LLM', 'using loss functions for LoRA and QLoRA in LLM fine-tuning', 'What are the best ways to use a loss functions when fine tuning an LLM model?']...\n",
      "INFO:     [17:28:05] \n",
      "üîç Running research for 'best practices for loss function selection in LLM fine-tuning'...\n",
      "INFO:     [17:28:05] \n",
      "üîç Running research for 'LLM fine-tuning overfitting prevention methods with loss functions'...\n",
      "INFO:     [17:28:05] \n",
      "üîç Running research for 'parameter efficient fine-tuning and loss functions for LLM'...\n",
      "INFO:     [17:28:05] \n",
      "üîç Running research for 'using loss functions for LoRA and QLoRA in LLM fine-tuning'...\n",
      "INFO:     [17:28:05] \n",
      "üîç Running research for 'What are the best ways to use a loss functions when fine tuning an LLM model?'...\n",
      "INFO:     [17:28:08] ‚úÖ Added source url to research: https://arxiv.org/html/2408.13296v1\n",
      "\n",
      "INFO:     [17:28:08] ‚úÖ Added source url to research: https://www.geeksforgeeks.org/fine-tuning-large-language-model-llm/\n",
      "\n",
      "INFO:     [17:28:08] ‚úÖ Added source url to research: https://www.lakera.ai/blog/llm-fine-tuning-guide\n",
      "\n",
      "INFO:     [17:28:08] ‚úÖ Added source url to research: https://medium.com/@gobishangar11/llama-2-a-detailed-guide-to-fine-tuning-the-large-language-model-8968f77bcd15\n",
      "\n",
      "INFO:     [17:28:08] ‚úÖ Added source url to research: https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07\n",
      "\n",
      "INFO:     [17:28:08] ü§î Researching for relevant information across multiple sources...\n",
      "\n",
      "INFO:     [17:28:08] üåê Scraping content from 5 URLs...\n",
      "INFO:     [17:28:09] üìÑ Scraped 4 pages of content\n",
      "INFO:     [17:28:09] üñºÔ∏è Selected 0 new images from 0 total images\n",
      "INFO:     [17:28:09] üåê Scraping complete\n",
      "INFO:     [17:28:09] üìö Getting relevant content based on query: What are the best ways to use a loss functions when fine tuning an LLM model?...\n",
      "INFO:     [17:28:09] ‚úÖ Added source url to research: https://llmmodels.org/blog/llm-fine-tuning-guide-to-hitl-and-best-practices/\n",
      "\n",
      "INFO:     [17:28:09] ‚úÖ Added source url to research: https://medium.com/ai-native-publication/fine-tuning-large-language-models-strategies-challenges-and-best-practices-5cbf6dd1b8de\n",
      "\n",
      "INFO:     [17:28:09] ‚úÖ Added source url to research: https://arxiv.org/abs/2408.13296\n",
      "\n",
      "INFO:     [17:28:09] ü§î Researching for relevant information across multiple sources...\n",
      "\n",
      "INFO:     [17:28:09] üåê Scraping content from 3 URLs...\n",
      "INFO:     [17:28:10] üìÑ Scraped 2 pages of content\n",
      "INFO:     [17:28:10] üñºÔ∏è Selected 0 new images from 0 total images\n",
      "INFO:     [17:28:10] üåê Scraping complete\n",
      "INFO:     [17:28:10] üìö Getting relevant content based on query: best practices for loss function selection in LLM fine-tuning...\n",
      "INFO:     [17:28:10] ‚úÖ Added source url to research: https://www.ibm.com/think/topics/parameter-efficient-fine-tuning\n",
      "\n",
      "INFO:     [17:28:10] ‚úÖ Added source url to research: https://markovate.com/blog/parameter-efficient-fine-tuning-peft-of-llms-a-practical-guide/\n",
      "\n",
      "INFO:     [17:28:10] ‚úÖ Added source url to research: https://kanerika.com/blogs/parameter-efficient-fine-tuning/\n",
      "\n",
      "INFO:     [17:28:10] ‚úÖ Added source url to research: https://www.analyticsvidhya.com/blog/2023/10/llm-fine-tuning-with-peft-techniques/\n",
      "\n",
      "INFO:     [17:28:10] ‚úÖ Added source url to research: https://medium.com/@shivansh.kaushik/efficient-model-fine-tuning-for-llms-understanding-peft-by-implementation-fc4d5e985389\n",
      "\n",
      "INFO:     [17:28:10] ü§î Researching for relevant information across multiple sources...\n",
      "\n",
      "INFO:     [17:28:10] üåê Scraping content from 5 URLs...\n",
      "INFO:     [17:28:13] üìÑ Scraped 5 pages of content\n",
      "INFO:     [17:28:13] üñºÔ∏è Selected 4 new images from 10 total images\n",
      "INFO:     [17:28:13] üåê Scraping complete\n",
      "INFO:     [17:28:13] üìö Getting relevant content based on query: parameter efficient fine-tuning and loss functions for LLM...\n",
      "INFO:     [17:28:13] ‚úÖ Added source url to research: https://www.akaike.ai/resources/all-you-need-to-know-about-llm-fine-tuning-part-2\n",
      "\n",
      "INFO:     [17:28:14] ‚úÖ Added source url to research: https://labelyourdata.com/articles/llm-fine-tuning\n",
      "\n",
      "INFO:     [17:28:14] ‚úÖ Added source url to research: https://medium.com/ubiai-nlp/fine-tuning-llm-a-deep-dive-into-advanced-techniques-for-optimal-model-performance-289affdfaf61\n",
      "\n",
      "INFO:     [17:28:14] ‚úÖ Added source url to research: https://www.turing.com/resources/finetuning-large-language-models\n",
      "\n",
      "INFO:     [17:28:14] ü§î Researching for relevant information across multiple sources...\n",
      "\n",
      "INFO:     [17:28:14] üåê Scraping content from 4 URLs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing dimension value 49.72: invalid literal for int() with base 10: '49.72'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     [17:28:15] üìÑ Scraped 4 pages of content\n",
      "INFO:     [17:28:15] üñºÔ∏è Selected 1 new images from 1 total images\n",
      "INFO:     [17:28:15] üåê Scraping complete\n",
      "INFO:     [17:28:15] üìö Getting relevant content based on query: LLM fine-tuning overfitting prevention methods with loss functions...\n",
      "INFO:     [17:28:15] ‚úÖ Added source url to research: https://blog.lancedb.com/optimizing-llms-a-step-by-step-guide-to-fine-tuning-with-peft-and-qlora-22eddd13d25b/\n",
      "\n",
      "INFO:     [17:28:15] ‚úÖ Added source url to research: https://medium.com/data-science-in-your-pocket/lora-for-fine-tuning-llms-explained-with-codes-and-example-62a7ac5a3578\n",
      "\n",
      "INFO:     [17:28:15] ‚úÖ Added source url to research: https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora\n",
      "\n",
      "INFO:     [17:28:15] ü§î Researching for relevant information across multiple sources...\n",
      "\n",
      "INFO:     [17:28:15] üåê Scraping content from 3 URLs...\n",
      "INFO:     [17:28:16] üìÑ Scraped 3 pages of content\n",
      "INFO:     [17:28:16] üñºÔ∏è Selected 0 new images from 0 total images\n",
      "INFO:     [17:28:16] üåê Scraping complete\n",
      "INFO:     [17:28:16] üìö Getting relevant content based on query: using loss functions for LoRA and QLoRA in LLM fine-tuning...\n",
      "INFO:     [17:28:16] üìÉ Source: https://llmmodels.org/blog/llm-fine-tuning-guide-to-hitl-and-best-practices/\n",
      "Title: LLM Fine-Tuning: Guide to HITL & Best Practices\n",
      "Content: Best Practices for Fine-Tuning LLMs\n",
      "Fine-tuning Large Language Models (LLMs) requires careful consideration of several critical factors to achieve accurate and reliable outcomes. In this section, we will outline the best practices for fine-tuning LLMs, including data curation, model selection, iterative training, validation, and the importance of continuous refinement.\n",
      "High-quality data is essential for fine-tuning LLMs. The quality of the training dataset directly impacts the model's performance and bias. A well-curated dataset should be representative of the task at hand, diverse, and free from noise and errors.\n",
      "Data Curation Checklist\n",
      "Hyperparameters play a critical role in fine-tuning LLMs. Learning rate, batch size, and number of epochs are some of the most important hyperparameters that require careful tuning.\n",
      "Hyperparameter Tuning Strategies\n",
      "Model Selection and Modification\n",
      "\n",
      "Source: https://llmmodels.org/blog/llm-fine-tuning-guide-to-hitl-and-best-practices/\n",
      "Title: LLM Fine-Tuning: Guide to HITL & Best Practices\n",
      "Content: fine-tuning methodologies HITL fine-tuning offers a powerful way to enhance LLM performance and adaptability, but it requires careful consideration of data quality, ethical implications, and resource management. As the field evolves, we can expect more specialized, capable, and trustworthy LLMs that positively impact various domains. Related video from YouTube Challenges in Fine-Tuning LLMs Fine-tuning Large Language Models (LLMs) can be a complex task. Several obstacles can arise during the fine-tuning process, hindering the model's performance and accuracy. In this section, we will discuss some common challenges encountered when fine-tuning LLMs and explore strategies to overcome them. Data Quality and Quantity Issues One of the primary challenges in fine-tuning LLMs is sourcing high-quality and relevant training data. The quality of the training data directly impacts the model's performance, and low-quality data can lead to suboptimal results. Moreover, the quantity of data is also\n",
      "\n",
      "Source: https://llmmodels.org/blog/llm-fine-tuning-guide-to-hitl-and-best-practices/\n",
      "Title: LLM Fine-Tuning: Guide to HITL & Best Practices\n",
      "Content: fine-tuning methodologies HITL fine-tuning offers a powerful way to enhance LLM performance and adaptability, but it requires careful consideration of data quality, ethical implications, and resource management. As the field evolves, we can expect more specialized, capable, and trustworthy LLMs that positively impact various domains. Related video from YouTube Challenges in Fine-Tuning LLMs Fine-tuning Large Language Models (LLMs) can be a complex task. Several obstacles can arise during the fine-tuning process, hindering the model's performance and accuracy. In this section, we will discuss some common challenges encountered when fine-tuning LLMs and explore strategies to overcome them. Data Quality and Quantity Issues One of the primary challenges in fine-tuning LLMs is sourcing high-quality and relevant training data. The quality of the training data directly impacts the model's performance, and low-quality data can lead to suboptimal results. Moreover, the quantity of data is also\n",
      "\n",
      "Source: https://llmmodels.org/blog/llm-fine-tuning-guide-to-hitl-and-best-practices/\n",
      "Title: LLM Fine-Tuning: Guide to HITL & Best Practices\n",
      "Content: Fine-Tune an LLM Model? Fine-tuning an LLM model involves the following steps: Step Description 1 Obtain a task-specific dataset 2 Preprocess the data 3 Initialize with pre-trained weights 4 Fine-tune on the dataset 5 Evaluate performance 6 Iterate and refine When to Fine-Tune LLMs? Fine-tuning LLMs is beneficial in the following scenarios: Scenario Description Task specialization Optimize the model for a specific task Domain adaptation Adapt the model to a specialized domain Data privacy Use a limited, proprietary dataset Performance boost Improve the model's performance on a specific task What is an Example of Human-in-the-Loop? Human-in-the-loop (HITL) fine-tuning involves human feedback and corrections to an LLM's outputs. For example: In the medical domain, medical professionals provide feedback on an LLM's diagnoses or treatment recommendations. In content moderation, human reviewers provide feedback on an LLM's generated text, flagging inappropriate or harmful content. This\n",
      "\n",
      "Source: https://llmmodels.org/blog/llm-fine-tuning-guide-to-hitl-and-best-practices/\n",
      "Title: LLM Fine-Tuning: Guide to HITL & Best Practices\n",
      "Content: Fine-Tune an LLM Model? Fine-tuning an LLM model involves the following steps: Step Description 1 Obtain a task-specific dataset 2 Preprocess the data 3 Initialize with pre-trained weights 4 Fine-tune on the dataset 5 Evaluate performance 6 Iterate and refine When to Fine-Tune LLMs? Fine-tuning LLMs is beneficial in the following scenarios: Scenario Description Task specialization Optimize the model for a specific task Domain adaptation Adapt the model to a specialized domain Data privacy Use a limited, proprietary dataset Performance boost Improve the model's performance on a specific task What is an Example of Human-in-the-Loop? Human-in-the-loop (HITL) fine-tuning involves human feedback and corrections to an LLM's outputs. For example: In the medical domain, medical professionals provide feedback on an LLM's diagnoses or treatment recommendations. In content moderation, human reviewers provide feedback on an LLM's generated text, flagging inappropriate or harmful content. This\n",
      "\n",
      "Source: https://llmmodels.org/blog/llm-fine-tuning-guide-to-hitl-and-best-practices/\n",
      "Title: LLM Fine-Tuning: Guide to HITL & Best Practices\n",
      "Content: Hyperparameter Tuning Strategies\n",
      "Model Selection and Modification\n",
      "Selecting the right pre-trained LLM and modifying it for the specific task at hand is crucial for fine-tuning.\n",
      "Model Selection and Modification Tips\n",
      "By following these best practices, developers can fine-tune LLMs that are accurate, reliable, and adaptable to specific tasks. Remember, fine-tuning is an iterative process that requires continuous refinement and evaluation to achieve optimal results.\n",
      "Using Human Feedback for Fine-Tuning\n",
      "Fine-tuning Large Language Models (LLMs) with human feedback, also known as Human-in-the-Loop (HITL), is a powerful approach to improve model performance and reliability. By incorporating human input into the fine-tuning process, developers can create more accurate models that better serve specific tasks.\n",
      "The HITL Process\n",
      "The HITL process involves the following steps:\n",
      "\n",
      "Source: https://llmmodels.org/blog/llm-fine-tuning-guide-to-hitl-and-best-practices/\n",
      "Title: LLM Fine-Tuning: Guide to HITL & Best Practices\n",
      "Content: the model's performance on the validation set starts to degrade. Hyperparameter optimization Tunes the model's hyperparameters to find the optimal combination that prevents overfitting. Computational Resource Management Fine-tuning LLMs can be computationally expensive, requiring significant resources and infrastructure. This can be a challenge, especially for organizations with limited resources. Alternative Description Model distillation Trains a smaller model to mimic the behavior of a larger, pre-trained model. Efficient architectures Designs models that are computationally efficient and require fewer resources. By understanding and addressing these challenges, developers and practitioners can fine-tune LLMs more effectively, achieving better performance and accuracy in their AI applications. Prompt Engineering for Fine-Tuning Prompt engineering is a technique that allows developers to customize Large Language Models (LLMs) for specific tasks without extensive fine-tuning. By\n",
      "\n",
      "Source: https://llmmodels.org/blog/llm-fine-tuning-guide-to-hitl-and-best-practices/\n",
      "Title: LLM Fine-Tuning: Guide to HITL & Best Practices\n",
      "Content: Fine-tuning LLMs is beneficial in the following scenarios:\n",
      "What is an Example of Human-in-the-Loop?\n",
      "Human-in-the-loop (HITL) fine-tuning involves human feedback and corrections to an LLM's outputs. For example:\n",
      "In the medical domain, medical professionals provide feedback on an LLM's diagnoses or treatment recommendations.\n",
      "In content moderation, human reviewers provide feedback on an LLM's generated text, flagging inappropriate or harmful content.\n",
      "This human input is used to fine-tune the model, enabling it to learn from expert knowledge and improve its performance.\n",
      "Unsupervised Pre-training vs. Supervised Fine-tuning for LLMs\n",
      "LLM Observability: Debugging with Arize Phoenix\n",
      "10 Hyperparameter Tuning Tips for LLM Fine-Tuning\n",
      "Top 10 Open-Source LLM Frameworks 2024\n",
      "LLM Models and Directory Submissions: Ensuring Web Visibility Exploring Open Source AI Models: LLMs and Transformer Architectures Fairseq Tutorial: Getting Started with Sequence-to-Sequence Models\n",
      "\n",
      "Source: https://llmmodels.org/blog/llm-fine-tuning-guide-to-hitl-and-best-practices/\n",
      "Title: LLM Fine-Tuning: Guide to HITL & Best Practices\n",
      "Content: of hyperparameters to find the best one Random search Randomly sample hyperparameters to find the best one Bayesian optimization Use Bayesian methods to find the optimal hyperparameters Model Selection and Modification Selecting the right pre-trained LLM and modifying it for the specific task at hand is crucial for fine-tuning. Model Selection and Modification Tips Tip Description Choose a relevant model Select a pre-trained model that aligns with the task requirements Modify the model architecture Adapt the model to incorporate task-specific knowledge Use transfer learning Leverage pre-trained models to fine-tune for the new task By following these best practices, developers can fine-tune LLMs that are accurate, reliable, and adaptable to specific tasks. Remember, fine-tuning is an iterative process that requires continuous refinement and evaluation to achieve optimal results. sbb-itb-f3e41df Using Human Feedback for Fine-Tuning Fine-tuning Large Language Models (LLMs) with human\n",
      "\n",
      "Source: https://llmmodels.org/blog/llm-fine-tuning-guide-to-hitl-and-best-practices/\n",
      "Title: LLM Fine-Tuning: Guide to HITL & Best Practices\n",
      "Content: prevent overfitting. Early stopping Stops the training process when the model's performance on the validation set starts to degrade. Hyperparameter optimization Tunes the model's hyperparameters to find the optimal combination that prevents overfitting. Computational Resource Management Fine-tuning LLMs can be computationally expensive, requiring significant resources and infrastructure. This can be a challenge, especially for organizations with limited resources. Alternative Description Model distillation Trains a smaller model to mimic the behavior of a larger, pre-trained model. Efficient architectures Designs models that are computationally efficient and require fewer resources. By understanding and addressing these challenges, developers and practitioners can fine-tune LLMs more effectively, achieving better performance and accuracy in their AI applications. Prompt Engineering for Fine-Tuning Prompt engineering is a technique that allows developers to customize Large Language\n",
      "\n",
      "INFO:     [17:28:21] üìÉ Source: https://www.akaike.ai/resources/all-you-need-to-know-about-llm-fine-tuning-part-2\n",
      "Title: All You Need to Know About LLM Fine-Tuning (Part 2) | Akaike Ai\n",
      "Content: to ensure that your fine-tuning efforts are not only successful but also sustainable in the long run. Whether you√¢¬Ä¬ôre a seasoned machine learning practitioner or just beginning your journey, this article aims to provide you with actionable insights to master the art of fine-tuning LLMs.Popular Methods for LLM Fine-TuningFine-tuning LLMs involves tailoring a general model to specific tasks, and selecting the right method is essential for optimal performance. Methods like transfer learning adapt a broad model to related tasks (e.g., using a general language model for sentiment analysis of reviews), while task-specific fine-tuning focuses on training a model for particular applications (e.g., customizing a model for legal documents or medical terminology). Choosing the right method ensures the model performs well for its intended application. For example, fine-tuning with conversational data can improve a chatbot's understanding of customer queries, while task-specific fine-tuning might\n",
      "\n",
      "Source: https://www.akaike.ai/resources/all-you-need-to-know-about-llm-fine-tuning-part-2\n",
      "Title: All You Need to Know About LLM Fine-Tuning (Part 2) | Akaike Ai\n",
      "Content: to ensure that your fine-tuning efforts are not only successful but also sustainable in the long run. Whether you√¢¬Ä¬ôre a seasoned machine learning practitioner or just beginning your journey, this article aims to provide you with actionable insights to master the art of fine-tuning LLMs.Popular Methods for LLM Fine-TuningFine-tuning LLMs involves tailoring a general model to specific tasks, and selecting the right method is essential for optimal performance. Methods like transfer learning adapt a broad model to related tasks (e.g., using a general language model for sentiment analysis of reviews), while task-specific fine-tuning focuses on training a model for particular applications (e.g., customizing a model for legal documents or medical terminology). Choosing the right method ensures the model performs well for its intended application. For example, fine-tuning with conversational data can improve a chatbot's understanding of customer queries, while task-specific fine-tuning might\n",
      "\n",
      "Source: https://www.akaike.ai/resources/all-you-need-to-know-about-llm-fine-tuning-part-2\n",
      "Title: All You Need to Know About LLM Fine-Tuning (Part 2) | Akaike Ai\n",
      "Content: to ensure that your fine-tuning efforts are not only successful but also sustainable in the long run. Whether you√¢¬Ä¬ôre a seasoned machine learning practitioner or just beginning your journey, this article aims to provide you with actionable insights to master the art of fine-tuning LLMs.Popular Methods for LLM Fine-TuningFine-tuning LLMs involves tailoring a general model to specific tasks, and selecting the right method is essential for optimal performance. Methods like transfer learning adapt a broad model to related tasks (e.g., using a general language model for sentiment analysis of reviews), while task-specific fine-tuning focuses on training a model for particular applications (e.g., customizing a model for legal documents or medical terminology). Choosing the right method ensures the model performs well for its intended application. For example, fine-tuning with conversational data can improve a chatbot's understanding of customer queries, while task-specific fine-tuning might\n",
      "\n",
      "Source: https://www.akaike.ai/resources/all-you-need-to-know-about-llm-fine-tuning-part-2\n",
      "Title: All You Need to Know About LLM Fine-Tuning (Part 2) | Akaike Ai\n",
      "Content: to ensure that your fine-tuning efforts are not only successful but also sustainable in the long run. Whether you√¢¬Ä¬ôre a seasoned machine learning practitioner or just beginning your journey, this article aims to provide you with actionable insights to master the art of fine-tuning LLMs.Popular Methods for LLM Fine-TuningFine-tuning LLMs involves tailoring a general model to specific tasks, and selecting the right method is essential for optimal performance. Methods like transfer learning adapt a broad model to related tasks (e.g., using a general language model for sentiment analysis of reviews), while task-specific fine-tuning focuses on training a model for particular applications (e.g., customizing a model for legal documents or medical terminology). Choosing the right method ensures the model performs well for its intended application. For example, fine-tuning with conversational data can improve a chatbot's understanding of customer queries, while task-specific fine-tuning might\n",
      "\n",
      "Source: https://www.akaike.ai/resources/all-you-need-to-know-about-llm-fine-tuning-part-2\n",
      "Title: All You Need to Know About LLM Fine-Tuning (Part 2) | Akaike Ai\n",
      "Content: to ensure that your fine-tuning efforts are not only successful but also sustainable in the long run. Whether you√¢¬Ä¬ôre a seasoned machine learning practitioner or just beginning your journey, this article aims to provide you with actionable insights to master the art of fine-tuning LLMs.Popular Methods for LLM Fine-TuningFine-tuning LLMs involves tailoring a general model to specific tasks, and selecting the right method is essential for optimal performance. Methods like transfer learning adapt a broad model to related tasks (e.g., using a general language model for sentiment analysis of reviews), while task-specific fine-tuning focuses on training a model for particular applications (e.g., customizing a model for legal documents or medical terminology). Choosing the right method ensures the model performs well for its intended application. For example, fine-tuning with conversational data can improve a chatbot's understanding of customer queries, while task-specific fine-tuning might\n",
      "\n",
      "Source: https://www.akaike.ai/resources/all-you-need-to-know-about-llm-fine-tuning-part-2\n",
      "Title: All You Need to Know About LLM Fine-Tuning (Part 2) | Akaike Ai\n",
      "Content: practices and strategic approaches to ensure that your fine-tuning efforts are not only successful but also sustainable in the long run. Whether you√¢¬Ä¬ôre a seasoned machine learning practitioner or just beginning your journey, this article aims to provide you with actionable insights to master the art of fine-tuning LLMs.Popular Methods for LLM Fine-TuningFine-tuning LLMs involves tailoring a general model to specific tasks, and selecting the right method is essential for optimal performance. Methods like transfer learning adapt a broad model to related tasks (e.g., using a general language model for sentiment analysis of reviews), while task-specific fine-tuning focuses on training a model for particular applications (e.g., customizing a model for legal documents or medical terminology). Choosing the right method ensures the model performs well for its intended application. For example, fine-tuning with conversational data can improve a chatbot's understanding of customer queries,\n",
      "\n",
      "Source: https://www.akaike.ai/resources/all-you-need-to-know-about-llm-fine-tuning-part-2\n",
      "Title: All You Need to Know About LLM Fine-Tuning (Part 2) | Akaike Ai\n",
      "Content: truly remarkable, specialized models? This blog will guide you through methods, best practices and strategic approaches to ensure that your fine-tuning efforts are not only successful but also sustainable in the long run. Whether you√¢¬Ä¬ôre a seasoned machine learning practitioner or just beginning your journey, this article aims to provide you with actionable insights to master the art of fine-tuning LLMs.Popular Methods for LLM Fine-TuningFine-tuning LLMs involves tailoring a general model to specific tasks, and selecting the right method is essential for optimal performance. Methods like transfer learning adapt a broad model to related tasks (e.g., using a general language model for sentiment analysis of reviews), while task-specific fine-tuning focuses on training a model for particular applications (e.g., customizing a model for legal documents or medical terminology). Choosing the right method ensures the model performs well for its intended application. For example, fine-tuning\n",
      "\n",
      "Source: https://labelyourdata.com/articles/llm-fine-tuning\n",
      "Title: LLM Fine Tuning: The 2024 Guide for ML Teams | Label Your Data\n",
      "Content: LLM fine-tuning is evolving with new techniques that make models more adaptable and efficient. Key developments include zero-shot and few-shot learning, which allow models to perform tasks with little to no task-specific data. These techniques enhance model generalization, but challenges like overfitting and the need for robust evaluation frameworks remain.Zero-Shot and Few-Shot Fine-TuningZero-shot and few-shot fine-tuning enable LLMs to handle tasks with minimal data. While zero-shot learning allows models to perform without prior examples, few-shot learning requires just a handful. Despite their potential, these methods face challenges like overfitting.Prompt Tuning and Hybrid ApproachesPrompt tuning, which refines the input prompts to improve model responses, is increasingly combined with traditional fine-tuning for enhanced performance. Hybrid approaches, blending prompt tuning with parameter-efficient fine-tuning methods, offer a balance between performance and computational\n",
      "\n",
      "Source: https://labelyourdata.com/articles/llm-fine-tuning\n",
      "Title: LLM Fine Tuning: The 2024 Guide for ML Teams | Label Your Data\n",
      "Content: LLM fine-tuning is evolving with new techniques that make models more adaptable and efficient. Key developments include zero-shot and few-shot learning, which allow models to perform tasks with little to no task-specific data. These techniques enhance model generalization, but challenges like overfitting and the need for robust evaluation frameworks remain.Zero-Shot and Few-Shot Fine-TuningZero-shot and few-shot fine-tuning enable LLMs to handle tasks with minimal data. While zero-shot learning allows models to perform without prior examples, few-shot learning requires just a handful. Despite their potential, these methods face challenges like overfitting.Prompt Tuning and Hybrid ApproachesPrompt tuning, which refines the input prompts to improve model responses, is increasingly combined with traditional fine-tuning for enhanced performance. Hybrid approaches, blending prompt tuning with parameter-efficient fine-tuning methods, offer a balance between performance and computational\n",
      "\n",
      "Source: https://labelyourdata.com/articles/llm-fine-tuning\n",
      "Title: LLM Fine Tuning: The 2024 Guide for ML Teams | Label Your Data\n",
      "Content: LLM fine-tuning is evolving with new techniques that make models more adaptable and efficient. Key developments include zero-shot and few-shot learning, which allow models to perform tasks with little to no task-specific data. These techniques enhance model generalization, but challenges like overfitting and the need for robust evaluation frameworks remain.Zero-Shot and Few-Shot Fine-TuningZero-shot and few-shot fine-tuning enable LLMs to handle tasks with minimal data. While zero-shot learning allows models to perform without prior examples, few-shot learning requires just a handful. Despite their potential, these methods face challenges like overfitting.Prompt Tuning and Hybrid ApproachesPrompt tuning, which refines the input prompts to improve model responses, is increasingly combined with traditional fine-tuning for enhanced performance. Hybrid approaches, blending prompt tuning with parameter-efficient fine-tuning methods, offer a balance between performance and computational\n",
      "\n",
      "INFO:     [17:28:21] üìÉ Source: https://www.lakera.ai/blog/llm-fine-tuning-guide\n",
      "Title: The Ultimate Guide to LLM Fine Tuning: Best Practices & Tools | Lakera √¢¬Ä¬ì Protecting AI teams that disrupt the world.\n",
      "Content: Language Models Fine Tuning MethodsLarge Language Models (LLMs) Fine Tuning Methods encompass a variety of techniques, ranging from traditional, time-tested approaches to innovative, cutting-edge strategies, all aimed at enhancing the performance and applicability of these powerful models in diverse contexts.Early Fine-tuning Methods / Old-schoolIn old-school approaches, there are various methods to fine tune pre-trained language models, each tailored to specific needs and resource constraints.Feature-based: It uses a pre-trained LLM as a feature extractor, transforming input text into a fixed-sized array. A separate classifier network predicts the text's class probability in NLP tasks. In training, only the classifier's weights change, making it resource-friendly but potentially less performant.Finetuning I: Finetuning I enhances the pre-trained LLM by adding extra dense layers. During training, only the newly added layers' weights are adjusted while keeping the pre-trained LLM\n",
      "\n",
      "Source: https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07\n",
      "Title: Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA | by Suman Das | Medium\n",
      "Content: the model to the nuances of the target domain.Fine-tuning LLMs is commonly used in natural language processing tasks such as sentiment analysis, named entity recognition, summarization, translation, or any other application where understanding context and generating coherent language is crucial. It helps leverage the knowledge encoded in pre-trained models for more specialized and domain-specific tasks.Fine-tuning methodsFine-tuning a Large Language Model (LLM) involves a supervised learning process. In this method, a dataset comprising labeled examples is utilized to adjust the model‚Äôs weights, enhancing its proficiency in specific tasks. Now, let‚Äôs delve into some noteworthy techniques employed in the fine-tuning process.Full Fine Tuning (Instruction fine-tuning): Instruction fine-tuning is a strategy to enhance a model‚Äôs performance across various tasks by training it on examples that guide its responses to queries. The choice of the dataset is crucial and tailored to the specific\n",
      "\n",
      "Source: https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07\n",
      "Title: Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA | by Suman Das | Medium\n",
      "Content: the model to the nuances of the target domain.Fine-tuning LLMs is commonly used in natural language processing tasks such as sentiment analysis, named entity recognition, summarization, translation, or any other application where understanding context and generating coherent language is crucial. It helps leverage the knowledge encoded in pre-trained models for more specialized and domain-specific tasks.Fine-tuning methodsFine-tuning a Large Language Model (LLM) involves a supervised learning process. In this method, a dataset comprising labeled examples is utilized to adjust the model‚Äôs weights, enhancing its proficiency in specific tasks. Now, let‚Äôs delve into some noteworthy techniques employed in the fine-tuning process.Full Fine Tuning (Instruction fine-tuning): Instruction fine-tuning is a strategy to enhance a model‚Äôs performance across various tasks by training it on examples that guide its responses to queries. The choice of the dataset is crucial and tailored to the specific\n",
      "\n",
      "Source: https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07\n",
      "Title: Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA | by Suman Das | Medium\n",
      "Content: the model to the nuances of the target domain.Fine-tuning LLMs is commonly used in natural language processing tasks such as sentiment analysis, named entity recognition, summarization, translation, or any other application where understanding context and generating coherent language is crucial. It helps leverage the knowledge encoded in pre-trained models for more specialized and domain-specific tasks.Fine-tuning methodsFine-tuning a Large Language Model (LLM) involves a supervised learning process. In this method, a dataset comprising labeled examples is utilized to adjust the model‚Äôs weights, enhancing its proficiency in specific tasks. Now, let‚Äôs delve into some noteworthy techniques employed in the fine-tuning process.Full Fine Tuning (Instruction fine-tuning): Instruction fine-tuning is a strategy to enhance a model‚Äôs performance across various tasks by training it on examples that guide its responses to queries. The choice of the dataset is crucial and tailored to the specific\n",
      "\n",
      "Source: https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07\n",
      "Title: Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA | by Suman Das | Medium\n",
      "Content: the model to the nuances of the target domain.Fine-tuning LLMs is commonly used in natural language processing tasks such as sentiment analysis, named entity recognition, summarization, translation, or any other application where understanding context and generating coherent language is crucial. It helps leverage the knowledge encoded in pre-trained models for more specialized and domain-specific tasks.Fine-tuning methodsFine-tuning a Large Language Model (LLM) involves a supervised learning process. In this method, a dataset comprising labeled examples is utilized to adjust the model‚Äôs weights, enhancing its proficiency in specific tasks. Now, let‚Äôs delve into some noteworthy techniques employed in the fine-tuning process.Full Fine Tuning (Instruction fine-tuning): Instruction fine-tuning is a strategy to enhance a model‚Äôs performance across various tasks by training it on examples that guide its responses to queries. The choice of the dataset is crucial and tailored to the specific\n",
      "\n",
      "Source: https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07\n",
      "Title: Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA | by Suman Das | Medium\n",
      "Content: the model to the nuances of the target domain.Fine-tuning LLMs is commonly used in natural language processing tasks such as sentiment analysis, named entity recognition, summarization, translation, or any other application where understanding context and generating coherent language is crucial. It helps leverage the knowledge encoded in pre-trained models for more specialized and domain-specific tasks.Fine-tuning methodsFine-tuning a Large Language Model (LLM) involves a supervised learning process. In this method, a dataset comprising labeled examples is utilized to adjust the model‚Äôs weights, enhancing its proficiency in specific tasks. Now, let‚Äôs delve into some noteworthy techniques employed in the fine-tuning process.Full Fine Tuning (Instruction fine-tuning): Instruction fine-tuning is a strategy to enhance a model‚Äôs performance across various tasks by training it on examples that guide its responses to queries. The choice of the dataset is crucial and tailored to the specific\n",
      "\n",
      "Source: https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07\n",
      "Title: Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA | by Suman Das | Medium\n",
      "Content: the model to the nuances of the target domain.Fine-tuning LLMs is commonly used in natural language processing tasks such as sentiment analysis, named entity recognition, summarization, translation, or any other application where understanding context and generating coherent language is crucial. It helps leverage the knowledge encoded in pre-trained models for more specialized and domain-specific tasks.Fine-tuning methodsFine-tuning a Large Language Model (LLM) involves a supervised learning process. In this method, a dataset comprising labeled examples is utilized to adjust the model‚Äôs weights, enhancing its proficiency in specific tasks. Now, let‚Äôs delve into some noteworthy techniques employed in the fine-tuning process.Full Fine Tuning (Instruction fine-tuning): Instruction fine-tuning is a strategy to enhance a model‚Äôs performance across various tasks by training it on examples that guide its responses to queries. The choice of the dataset is crucial and tailored to the specific\n",
      "\n",
      "Source: https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07\n",
      "Title: Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA | by Suman Das | Medium\n",
      "Content: the model to the nuances of the target domain.Fine-tuning LLMs is commonly used in natural language processing tasks such as sentiment analysis, named entity recognition, summarization, translation, or any other application where understanding context and generating coherent language is crucial. It helps leverage the knowledge encoded in pre-trained models for more specialized and domain-specific tasks.Fine-tuning methodsFine-tuning a Large Language Model (LLM) involves a supervised learning process. In this method, a dataset comprising labeled examples is utilized to adjust the model‚Äôs weights, enhancing its proficiency in specific tasks. Now, let‚Äôs delve into some noteworthy techniques employed in the fine-tuning process.Full Fine Tuning (Instruction fine-tuning): Instruction fine-tuning is a strategy to enhance a model‚Äôs performance across various tasks by training it on examples that guide its responses to queries. The choice of the dataset is crucial and tailored to the specific\n",
      "\n",
      "Source: https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07\n",
      "Title: Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA | by Suman Das | Medium\n",
      "Content: the model to the nuances of the target domain.Fine-tuning LLMs is commonly used in natural language processing tasks such as sentiment analysis, named entity recognition, summarization, translation, or any other application where understanding context and generating coherent language is crucial. It helps leverage the knowledge encoded in pre-trained models for more specialized and domain-specific tasks.Fine-tuning methodsFine-tuning a Large Language Model (LLM) involves a supervised learning process. In this method, a dataset comprising labeled examples is utilized to adjust the model‚Äôs weights, enhancing its proficiency in specific tasks. Now, let‚Äôs delve into some noteworthy techniques employed in the fine-tuning process.Full Fine Tuning (Instruction fine-tuning): Instruction fine-tuning is a strategy to enhance a model‚Äôs performance across various tasks by training it on examples that guide its responses to queries. The choice of the dataset is crucial and tailored to the specific\n",
      "\n",
      "Source: https://www.lakera.ai/blog/llm-fine-tuning-guide\n",
      "Title: The Ultimate Guide to LLM Fine Tuning: Best Practices & Tools | Lakera √¢¬Ä¬ì Protecting AI teams that disrupt the world.\n",
      "Content: Challenges & limitations of LLM fine tuning\n",
      "Some of the main challenges and limitations associated with fine-tuning LLMs:\n",
      "Overfitting: Fine-tuning can be prone to overfitting, a condition where the model becomes overly specialized on the training data and performs poorly on unseen data. This risk is particularly pronounced when the task-specific dataset is small or not representative of the broader context.\n",
      "Catastrophic Forgetting: During fine-tuning for a specific task, the model may forget previously acquired general knowledge. This phenomenon, known as catastrophic forgetting, can impair the model's adaptability to diverse tasks.\n",
      "Bias Amplification: Pre-trained models inherit biases from their training data, which fine-tuning can inadvertently amplify when applied to task-specific data. This amplification may lead to biased predictions and outputs, potentially causing ethical concerns.\n",
      "\n",
      "INFO:     [17:28:22] üìÉ Source: https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora\n",
      "Title: In-depth guide to fine-tuning LLMs with LoRA and QLoRA\n",
      "Content: for QLoRA training.QLoRA vs Standard FinetuningIn the paper, researchers provide a very detailed comparison between QLoRA, LoRA, and Full Finetuning of a network.√¢¬Ä¬çAs you can see in the above table, there is no loss of performance whatsoever in the T5 model family upon training with QLoRA and even with Double Quantization, we don√¢¬Ä¬ôt see any major differences. One significant difference is the number of LoRA adapters required. In the paper, the authors mention that they needed more LoRA adapters for QLoRA finetuning, compared to normal LoRA finetuning. The authors suggest applying the LoRA adapters on all the linear transformer blocks along with the query, key, and value layers.√¢¬Ä¬çEven for the much bigger language models, performance remains the same:√¢¬Ä¬çHence the authors trained a model on top of the base LLaMA models, Guanaco. This is a 33 billion parameter model, trained on the OASST1 dataset. At the time of its release, it became a state-of-the-art model and achieved 99.3%\n",
      "\n",
      "Source: https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora\n",
      "Title: In-depth guide to fine-tuning LLMs with LoRA and QLoRA\n",
      "Content: for QLoRA training.QLoRA vs Standard FinetuningIn the paper, researchers provide a very detailed comparison between QLoRA, LoRA, and Full Finetuning of a network.√¢¬Ä¬çAs you can see in the above table, there is no loss of performance whatsoever in the T5 model family upon training with QLoRA and even with Double Quantization, we don√¢¬Ä¬ôt see any major differences. One significant difference is the number of LoRA adapters required. In the paper, the authors mention that they needed more LoRA adapters for QLoRA finetuning, compared to normal LoRA finetuning. The authors suggest applying the LoRA adapters on all the linear transformer blocks along with the query, key, and value layers.√¢¬Ä¬çEven for the much bigger language models, performance remains the same:√¢¬Ä¬çHence the authors trained a model on top of the base LLaMA models, Guanaco. This is a 33 billion parameter model, trained on the OASST1 dataset. At the time of its release, it became a state-of-the-art model and achieved 99.3%\n",
      "\n",
      "Source: https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora\n",
      "Title: In-depth guide to fine-tuning LLMs with LoRA and QLoRA\n",
      "Content: for QLoRA training.QLoRA vs Standard FinetuningIn the paper, researchers provide a very detailed comparison between QLoRA, LoRA, and Full Finetuning of a network.√¢¬Ä¬çAs you can see in the above table, there is no loss of performance whatsoever in the T5 model family upon training with QLoRA and even with Double Quantization, we don√¢¬Ä¬ôt see any major differences. One significant difference is the number of LoRA adapters required. In the paper, the authors mention that they needed more LoRA adapters for QLoRA finetuning, compared to normal LoRA finetuning. The authors suggest applying the LoRA adapters on all the linear transformer blocks along with the query, key, and value layers.√¢¬Ä¬çEven for the much bigger language models, performance remains the same:√¢¬Ä¬çHence the authors trained a model on top of the base LLaMA models, Guanaco. This is a 33 billion parameter model, trained on the OASST1 dataset. At the time of its release, it became a state-of-the-art model and achieved 99.3%\n",
      "\n",
      "Source: https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora\n",
      "Title: In-depth guide to fine-tuning LLMs with LoRA and QLoRA\n",
      "Content: for QLoRA training.QLoRA vs Standard FinetuningIn the paper, researchers provide a very detailed comparison between QLoRA, LoRA, and Full Finetuning of a network.√¢¬Ä¬çAs you can see in the above table, there is no loss of performance whatsoever in the T5 model family upon training with QLoRA and even with Double Quantization, we don√¢¬Ä¬ôt see any major differences. One significant difference is the number of LoRA adapters required. In the paper, the authors mention that they needed more LoRA adapters for QLoRA finetuning, compared to normal LoRA finetuning. The authors suggest applying the LoRA adapters on all the linear transformer blocks along with the query, key, and value layers.√¢¬Ä¬çEven for the much bigger language models, performance remains the same:√¢¬Ä¬çHence the authors trained a model on top of the base LLaMA models, Guanaco. This is a 33 billion parameter model, trained on the OASST1 dataset. At the time of its release, it became a state-of-the-art model and achieved 99.3%\n",
      "\n",
      "Source: https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora\n",
      "Title: In-depth guide to fine-tuning LLMs with LoRA and QLoRA\n",
      "Content: for QLoRA training.QLoRA vs Standard FinetuningIn the paper, researchers provide a very detailed comparison between QLoRA, LoRA, and Full Finetuning of a network.√¢¬Ä¬çAs you can see in the above table, there is no loss of performance whatsoever in the T5 model family upon training with QLoRA and even with Double Quantization, we don√¢¬Ä¬ôt see any major differences. One significant difference is the number of LoRA adapters required. In the paper, the authors mention that they needed more LoRA adapters for QLoRA finetuning, compared to normal LoRA finetuning. The authors suggest applying the LoRA adapters on all the linear transformer blocks along with the query, key, and value layers.√¢¬Ä¬çEven for the much bigger language models, performance remains the same:√¢¬Ä¬çHence the authors trained a model on top of the base LLaMA models, Guanaco. This is a 33 billion parameter model, trained on the OASST1 dataset. At the time of its release, it became a state-of-the-art model and achieved 99.3%\n",
      "\n",
      "Source: https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora\n",
      "Title: In-depth guide to fine-tuning LLMs with LoRA and QLoRA\n",
      "Content: QLoRA vs Standard Finetuning\n",
      "In the paper, researchers provide a very detailed comparison between QLoRA, LoRA, and Full Finetuning of a network.\n",
      "As you can see in the above table, there is no loss of performance whatsoever in the T5 model family upon training with QLoRA and even with Double Quantization, we don√¢¬Ä¬ôt see any major differences. One significant difference is the number of LoRA adapters required. In the paper, the authors mention that they needed more LoRA adapters for QLoRA finetuning, compared to normal LoRA finetuning. The authors suggest applying the LoRA adapters on all the linear transformer blocks along with the query, key, and value layers.\n",
      "Even for the much bigger language models, performance remains the same:\n",
      "\n",
      "Source: https://blog.lancedb.com/optimizing-llms-a-step-by-step-guide-to-fine-tuning-with-peft-and-qlora-22eddd13d25b/\n",
      "Title: Optimizing LLMs: A Step-by-Step Guide to Fine-Tuning with PEFT and QLoRA\n",
      "Content: Performance: LoRA‚Äôs efficiency doesn‚Äôt come at the cost of performance. While the reduction in parameters might lead to slightly lower performance gains compared to full fine-tuning, LoRA still delivers impressive results, especially when compared to the base LLM model.In practice, LoRA is an invaluable tool for efficiently fine-tuning LLMs and adapting them to specific tasks without overwhelming computational and memory resources. It strikes a balance between parameter efficiency and performance, making it a go-to technique for many natural language processing applications.But wait, there‚Äôs a game-changer on the horizon ‚Äî QLoRA.What Is QLoRA?QLoRA, which stands for Quantized Low-rank Adaptation, takes fine-tuning to the next level. It empowers you to fine-tune LLMs on a single GPU, pushing the boundaries of what‚Äôs possible. How does QLoRA differ from LoRA?The paper introduces QLoRA, an efficient fine-tuning method that enables the training of a 65-billion-parameter language model on\n",
      "\n",
      "Source: https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora\n",
      "Title: In-depth guide to fine-tuning LLMs with LoRA and QLoRA\n",
      "Content: have lesser models like Vicuna 13B, and Guanaco33B because of the use of 4-bit precision used less memory than the 13B model.√¢¬Ä¬çOther variants of LoRA finetuningQA LoRAQA LoRA is another fine-tuning technique built on top of QLoRA, introduced in this paper. QALoRA was mainly released for finetuning diffusion models, but can easily be generalized for training any type of models, just like LoRA.√¢¬Ä¬çThe difference between QLoRA and QALoRA is that QALoRA is quantization aware meaning the weights of the LoRA adapters are also quantized along with the weights of the model during the finetuning process. This helps in more efficient training as there is no need for the conversion step to update the models during the backpropagation process.√¢¬Ä¬çLongLoRA√¢¬Ä¬çLongLoRA is yet another variation of the LoRA finetuning technique, but this technique is specifically for training longer context models. LongLoRA works by using something called SHIFT SHORT ATTENTION. This method creates chunks or groups of\n",
      "\n",
      "Source: https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora\n",
      "Title: In-depth guide to fine-tuning LLMs with LoRA and QLoRA\n",
      "Content: have lesser models like Vicuna 13B, and Guanaco33B because of the use of 4-bit precision used less memory than the 13B model.√¢¬Ä¬çOther variants of LoRA finetuningQA LoRAQA LoRA is another fine-tuning technique built on top of QLoRA, introduced in this paper. QALoRA was mainly released for finetuning diffusion models, but can easily be generalized for training any type of models, just like LoRA.√¢¬Ä¬çThe difference between QLoRA and QALoRA is that QALoRA is quantization aware meaning the weights of the LoRA adapters are also quantized along with the weights of the model during the finetuning process. This helps in more efficient training as there is no need for the conversion step to update the models during the backpropagation process.√¢¬Ä¬çLongLoRA√¢¬Ä¬çLongLoRA is yet another variation of the LoRA finetuning technique, but this technique is specifically for training longer context models. LongLoRA works by using something called SHIFT SHORT ATTENTION. This method creates chunks or groups of\n",
      "\n",
      "Source: https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora\n",
      "Title: In-depth guide to fine-tuning LLMs with LoRA and QLoRA\n",
      "Content: have lesser models like Vicuna 13B, and Guanaco33B because of the use of 4-bit precision used less memory than the 13B model.√¢¬Ä¬çOther variants of LoRA finetuningQA LoRAQA LoRA is another fine-tuning technique built on top of QLoRA, introduced in this paper. QALoRA was mainly released for finetuning diffusion models, but can easily be generalized for training any type of models, just like LoRA.√¢¬Ä¬çThe difference between QLoRA and QALoRA is that QALoRA is quantization aware meaning the weights of the LoRA adapters are also quantized along with the weights of the model during the finetuning process. This helps in more efficient training as there is no need for the conversion step to update the models during the backpropagation process.√¢¬Ä¬çLongLoRA√¢¬Ä¬çLongLoRA is yet another variation of the LoRA finetuning technique, but this technique is specifically for training longer context models. LongLoRA works by using something called SHIFT SHORT ATTENTION. This method creates chunks or groups of\n",
      "\n",
      "INFO:     [17:28:26] üìÉ Source: https://www.ibm.com/think/topics/parameter-efficient-fine-tuning\n",
      "Title: What is parameter-efficient fine-tuning (PEFT)?\n",
      "Content: Parameter-efficient fine-tuning brings a wealth of benefits that have made it popular with organizations that use LLMs in their work:\n",
      "Parameter-efficient fine-tuning brings a wealth of benefits that have made it popular with organizations that use LLMs in their work:\n",
      "Parameter-efficient fine-tuning brings a wealth of benefits that have made it popular with organizations that use LLMs in their work:\n",
      "Increased efficiency Most large language models used in generative AI (gen AI) are powered by expensive graphics processing units (GPUs) made by manufacturers such as Nvidia. Each LLM uses large amounts of computational resources and energy. Adjusting only the most relevant parameters imparts large savings on energy and cloud computing costs.\n",
      "\n",
      "Source: https://www.ibm.com/think/topics/parameter-efficient-fine-tuning\n",
      "Title: What is parameter-efficient fine-tuning (PEFT)?\n",
      "Content: Parameter-efficient fine-tuning brings a wealth of benefits that have made it popular with organizations that use LLMs in their work: Increased efficiency Most large language models used in generative AI (gen AI) are powered by expensive graphics processing units (GPUs) made by manufacturers such as Nvidia. Each LLM uses large amounts of computational resources and energy. Adjusting only the most relevant parameters imparts large savings on energy and cloud computing costs. Faster time-to-value Time-to-value is the amount of time that it takes to develop, train and deploy an LLM so it can begin generating value for the organization that uses it. Because PEFT tweaks only a few trainable parameters, it takes far less time to update a model for a new task. PEFT can deliver comparable performance to a full fine-tuning process at a fraction of the time and expense. No catastrophic forgetting Catastrophic forgetting happens when LLMs lose or ‚Äúforget‚Äù the knowledge gained during the initial\n",
      "\n",
      "Source: https://www.analyticsvidhya.com/blog/2023/10/llm-fine-tuning-with-peft-techniques/\n",
      "Title: The Ultimate Guide to Parameter-efficient Fine-tuning with PEFT\n",
      "Content: efficiency with acceptable error rates. Training Arguments: Define training arguments such as batch size, optimizer, learning rate scheduler, and checkpoints for your fine-tuning process. Fine-Tuning: Use the HuggingFace Trainer with your PEFT configuration to fine-tune your LLM. Monitor training progress using libraries like WandB. Validation: Keep an eye on both training and validation loss to ensure your model doesn‚Äôt overfit. Checkpointing: Save checkpoints to resume training from specific points if needed. Remember that fine-tuning an LLM, especially with PEFT, is a delicate balance between efficient parameter modification and maintaining model performance. Language Models and Fine-Tuning are powerful tools in the field of natural language processing. The PEFT technique, coupled with parameter efficiency strategies like LoRA and Quantization, allows us to make the most of these models efficiently. With the right configuration and careful training, we can unlock the true potential\n",
      "\n",
      "Source: https://www.analyticsvidhya.com/blog/2023/10/llm-fine-tuning-with-peft-techniques/\n",
      "Title: The Ultimate Guide to Parameter-efficient Fine-tuning with PEFT\n",
      "Content: for Overfitting Remember, overfitting is a common issue during fine-tuning. To detect it, you need to track both training loss and validation loss. If the training loss keeps decreasing while the validation loss starts increasing, it‚Äôs a sign of overfitting. Ensure you have a separate validation dataset and pass it to the Trainer to monitor validation loss. That‚Äôs it! You‚Äôve successfully set up your environment and coded the fine-tuning process for your LLM using the PEFT technique. By following this step-by-step guide and monitoring your model‚Äôs performance, you‚Äôll be well on your way to leveraging the power of LLMs for various natural language understanding tasks. Conclusion In this exploration of language models and fine-tuning, we‚Äôve delved into the intricacies of harnessing the potential of LLMs through the innovative PEFT technique. This transformative approach allows us to efficiently adapt large models like Falcon 7B for specific tasks while balancing computational resources.\n",
      "\n",
      "Source: https://www.analyticsvidhya.com/blog/2023/10/llm-fine-tuning-with-peft-techniques/\n",
      "Title: The Ultimate Guide to Parameter-efficient Fine-tuning with PEFT\n",
      "Content: for Overfitting Remember, overfitting is a common issue during fine-tuning. To detect it, you need to track both training loss and validation loss. If the training loss keeps decreasing while the validation loss starts increasing, it‚Äôs a sign of overfitting. Ensure you have a separate validation dataset and pass it to the Trainer to monitor validation loss. That‚Äôs it! You‚Äôve successfully set up your environment and coded the fine-tuning process for your LLM using the PEFT technique. By following this step-by-step guide and monitoring your model‚Äôs performance, you‚Äôll be well on your way to leveraging the power of LLMs for various natural language understanding tasks. Conclusion In this exploration of language models and fine-tuning, we‚Äôve delved into the intricacies of harnessing the potential of LLMs through the innovative PEFT technique. This transformative approach allows us to efficiently adapt large models like Falcon 7B for specific tasks while balancing computational resources.\n",
      "\n",
      "Source: https://medium.com/@shivansh.kaushik/efficient-model-fine-tuning-for-llms-understanding-peft-by-implementation-fc4d5e985389\n",
      "Title: Efficient Model Fine-Tuning for LLMs: Understanding PEFT by Implementation | by Shivansh Kaushik | Medium\n",
      "Content: Efficient Model Fine-Tuning for LLMs: Understanding PEFT by ImplementationShivansh Kaushik¬∑Follow7 min read¬∑Jul 31, 2023--ListenShareFine-tuning large language models (LLMs) is a computationally intensive process, posing challenges in terms of memory requirements and time consumption. To address these issues, researchers have developed Parameter Efficient Fine-Tuning (PEFT) methods, which optimize fine-tuning by updating only a small subset of model parameters. In this article, we explore the key concepts of PEFT and two prominent techniques, LoRA and Prompt Tuning, that offer efficient adaptation of LLMs for specific tasks.The Challenges of Full Fine-TuningLarge language models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks, but their success comes at a cost ‚Äî computational intensity. Full fine-tuning, the traditional method of adapting these models to specific tasks, requires updating all model weights during supervised learning. While this\n",
      "\n",
      "Source: https://medium.com/@shivansh.kaushik/efficient-model-fine-tuning-for-llms-understanding-peft-by-implementation-fc4d5e985389\n",
      "Title: Efficient Model Fine-Tuning for LLMs: Understanding PEFT by Implementation | by Shivansh Kaushik | Medium\n",
      "Content: Efficient Model Fine-Tuning for LLMs: Understanding PEFT by ImplementationShivansh Kaushik¬∑Follow7 min read¬∑Jul 31, 2023--ListenShareFine-tuning large language models (LLMs) is a computationally intensive process, posing challenges in terms of memory requirements and time consumption. To address these issues, researchers have developed Parameter Efficient Fine-Tuning (PEFT) methods, which optimize fine-tuning by updating only a small subset of model parameters. In this article, we explore the key concepts of PEFT and two prominent techniques, LoRA and Prompt Tuning, that offer efficient adaptation of LLMs for specific tasks.The Challenges of Full Fine-TuningLarge language models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks, but their success comes at a cost ‚Äî computational intensity. Full fine-tuning, the traditional method of adapting these models to specific tasks, requires updating all model weights during supervised learning. While this\n",
      "\n",
      "Source: https://medium.com/@shivansh.kaushik/efficient-model-fine-tuning-for-llms-understanding-peft-by-implementation-fc4d5e985389\n",
      "Title: Efficient Model Fine-Tuning for LLMs: Understanding PEFT by Implementation | by Shivansh Kaushik | Medium\n",
      "Content: Efficient Model Fine-Tuning for LLMs: Understanding PEFT by ImplementationShivansh Kaushik¬∑Follow7 min read¬∑Jul 31, 2023--ListenShareFine-tuning large language models (LLMs) is a computationally intensive process, posing challenges in terms of memory requirements and time consumption. To address these issues, researchers have developed Parameter Efficient Fine-Tuning (PEFT) methods, which optimize fine-tuning by updating only a small subset of model parameters. In this article, we explore the key concepts of PEFT and two prominent techniques, LoRA and Prompt Tuning, that offer efficient adaptation of LLMs for specific tasks.The Challenges of Full Fine-TuningLarge language models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks, but their success comes at a cost ‚Äî computational intensity. Full fine-tuning, the traditional method of adapting these models to specific tasks, requires updating all model weights during supervised learning. While this\n",
      "\n",
      "Source: https://medium.com/@shivansh.kaushik/efficient-model-fine-tuning-for-llms-understanding-peft-by-implementation-fc4d5e985389\n",
      "Title: Efficient Model Fine-Tuning for LLMs: Understanding PEFT by Implementation | by Shivansh Kaushik | Medium\n",
      "Content: Efficient Model Fine-Tuning for LLMs: Understanding PEFT by ImplementationShivansh Kaushik¬∑Follow7 min read¬∑Jul 31, 2023--ListenShareFine-tuning large language models (LLMs) is a computationally intensive process, posing challenges in terms of memory requirements and time consumption. To address these issues, researchers have developed Parameter Efficient Fine-Tuning (PEFT) methods, which optimize fine-tuning by updating only a small subset of model parameters. In this article, we explore the key concepts of PEFT and two prominent techniques, LoRA and Prompt Tuning, that offer efficient adaptation of LLMs for specific tasks.The Challenges of Full Fine-TuningLarge language models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks, but their success comes at a cost ‚Äî computational intensity. Full fine-tuning, the traditional method of adapting these models to specific tasks, requires updating all model weights during supervised learning. While this\n",
      "\n",
      "Source: https://medium.com/@shivansh.kaushik/efficient-model-fine-tuning-for-llms-understanding-peft-by-implementation-fc4d5e985389\n",
      "Title: Efficient Model Fine-Tuning for LLMs: Understanding PEFT by Implementation | by Shivansh Kaushik | Medium\n",
      "Content: Efficient Model Fine-Tuning for LLMs: Understanding PEFT by ImplementationShivansh Kaushik¬∑Follow7 min read¬∑Jul 31, 2023--ListenShareFine-tuning large language models (LLMs) is a computationally intensive process, posing challenges in terms of memory requirements and time consumption. To address these issues, researchers have developed Parameter Efficient Fine-Tuning (PEFT) methods, which optimize fine-tuning by updating only a small subset of model parameters. In this article, we explore the key concepts of PEFT and two prominent techniques, LoRA and Prompt Tuning, that offer efficient adaptation of LLMs for specific tasks.The Challenges of Full Fine-TuningLarge language models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks, but their success comes at a cost ‚Äî computational intensity. Full fine-tuning, the traditional method of adapting these models to specific tasks, requires updating all model weights during supervised learning. While this\n",
      "\n",
      "INFO:     [17:28:26] Finalized research step.\n",
      "üí∏ Total Research Costs: $0.03941438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Source: https://llmmodels.org/blog/llm-fine-tuning-guide-to-hitl-and-best-practices/\\nTitle: LLM Fine-Tuning: Guide to HITL & Best Practices\\nContent: Best Practices for Fine-Tuning LLMs\\nFine-tuning Large Language Models (LLMs) requires careful consideration of several critical factors to achieve accurate and reliable outcomes. In this section, we will outline the best practices for fine-tuning LLMs, including data curation, model selection, iterative training, validation, and the importance of continuous refinement.\\nHigh-quality data is essential for fine-tuning LLMs. The quality of the training dataset directly impacts the model's performance and bias. A well-curated dataset should be representative of the task at hand, diverse, and free from noise and errors.\\nData Curation Checklist\\nHyperparameters play a critical role in fine-tuning LLMs. Learning rate, batch size, and number of epochs are some of the most important hyperparameters that require careful tuning.\\nHyperparameter Tuning Strategies\\nModel Selection and Modification\\n\\nSource: https://llmmodels.org/blog/llm-fine-tuning-guide-to-hitl-and-best-practices/\\nTitle: LLM Fine-Tuning: Guide to HITL & Best Practices\\nContent: fine-tuning methodologies HITL fine-tuning offers a powerful way to enhance LLM performance and adaptability, but it requires careful consideration of data quality, ethical implications, and resource management. As the field evolves, we can expect more specialized, capable, and trustworthy LLMs that positively impact various domains. Related video from YouTube Challenges in Fine-Tuning LLMs Fine-tuning Large Language Models (LLMs) can be a complex task. Several obstacles can arise during the fine-tuning process, hindering the model's performance and accuracy. In this section, we will discuss some common challenges encountered when fine-tuning LLMs and explore strategies to overcome them. Data Quality and Quantity Issues One of the primary challenges in fine-tuning LLMs is sourcing high-quality and relevant training data. The quality of the training data directly impacts the model's performance, and low-quality data can lead to suboptimal results. Moreover, the quantity of data is also\\n\\nSource: https://llmmodels.org/blog/llm-fine-tuning-guide-to-hitl-and-best-practices/\\nTitle: LLM Fine-Tuning: Guide to HITL & Best Practices\\nContent: fine-tuning methodologies HITL fine-tuning offers a powerful way to enhance LLM performance and adaptability, but it requires careful consideration of data quality, ethical implications, and resource management. As the field evolves, we can expect more specialized, capable, and trustworthy LLMs that positively impact various domains. Related video from YouTube Challenges in Fine-Tuning LLMs Fine-tuning Large Language Models (LLMs) can be a complex task. Several obstacles can arise during the fine-tuning process, hindering the model's performance and accuracy. In this section, we will discuss some common challenges encountered when fine-tuning LLMs and explore strategies to overcome them. Data Quality and Quantity Issues One of the primary challenges in fine-tuning LLMs is sourcing high-quality and relevant training data. The quality of the training data directly impacts the model's performance, and low-quality data can lead to suboptimal results. Moreover, the quantity of data is also\\n\\nSource: https://llmmodels.org/blog/llm-fine-tuning-guide-to-hitl-and-best-practices/\\nTitle: LLM Fine-Tuning: Guide to HITL & Best Practices\\nContent: Fine-Tune an LLM Model? Fine-tuning an LLM model involves the following steps: Step Description 1 Obtain a task-specific dataset 2 Preprocess the data 3 Initialize with pre-trained weights 4 Fine-tune on the dataset 5 Evaluate performance 6 Iterate and refine When to Fine-Tune LLMs? Fine-tuning LLMs is beneficial in the following scenarios: Scenario Description Task specialization Optimize the model for a specific task Domain adaptation Adapt the model to a specialized domain Data privacy Use a limited, proprietary dataset Performance boost Improve the model's performance on a specific task What is an Example of Human-in-the-Loop? Human-in-the-loop (HITL) fine-tuning involves human feedback and corrections to an LLM's outputs. For example: In the medical domain, medical professionals provide feedback on an LLM's diagnoses or treatment recommendations. In content moderation, human reviewers provide feedback on an LLM's generated text, flagging inappropriate or harmful content. This\\n\\nSource: https://llmmodels.org/blog/llm-fine-tuning-guide-to-hitl-and-best-practices/\\nTitle: LLM Fine-Tuning: Guide to HITL & Best Practices\\nContent: Fine-Tune an LLM Model? Fine-tuning an LLM model involves the following steps: Step Description 1 Obtain a task-specific dataset 2 Preprocess the data 3 Initialize with pre-trained weights 4 Fine-tune on the dataset 5 Evaluate performance 6 Iterate and refine When to Fine-Tune LLMs? Fine-tuning LLMs is beneficial in the following scenarios: Scenario Description Task specialization Optimize the model for a specific task Domain adaptation Adapt the model to a specialized domain Data privacy Use a limited, proprietary dataset Performance boost Improve the model's performance on a specific task What is an Example of Human-in-the-Loop? Human-in-the-loop (HITL) fine-tuning involves human feedback and corrections to an LLM's outputs. For example: In the medical domain, medical professionals provide feedback on an LLM's diagnoses or treatment recommendations. In content moderation, human reviewers provide feedback on an LLM's generated text, flagging inappropriate or harmful content. This\\n\\nSource: https://llmmodels.org/blog/llm-fine-tuning-guide-to-hitl-and-best-practices/\\nTitle: LLM Fine-Tuning: Guide to HITL & Best Practices\\nContent: Hyperparameter Tuning Strategies\\nModel Selection and Modification\\nSelecting the right pre-trained LLM and modifying it for the specific task at hand is crucial for fine-tuning.\\nModel Selection and Modification Tips\\nBy following these best practices, developers can fine-tune LLMs that are accurate, reliable, and adaptable to specific tasks. Remember, fine-tuning is an iterative process that requires continuous refinement and evaluation to achieve optimal results.\\nUsing Human Feedback for Fine-Tuning\\nFine-tuning Large Language Models (LLMs) with human feedback, also known as Human-in-the-Loop (HITL), is a powerful approach to improve model performance and reliability. By incorporating human input into the fine-tuning process, developers can create more accurate models that better serve specific tasks.\\nThe HITL Process\\nThe HITL process involves the following steps:\\n\\nSource: https://llmmodels.org/blog/llm-fine-tuning-guide-to-hitl-and-best-practices/\\nTitle: LLM Fine-Tuning: Guide to HITL & Best Practices\\nContent: the model's performance on the validation set starts to degrade. Hyperparameter optimization Tunes the model's hyperparameters to find the optimal combination that prevents overfitting. Computational Resource Management Fine-tuning LLMs can be computationally expensive, requiring significant resources and infrastructure. This can be a challenge, especially for organizations with limited resources. Alternative Description Model distillation Trains a smaller model to mimic the behavior of a larger, pre-trained model. Efficient architectures Designs models that are computationally efficient and require fewer resources. By understanding and addressing these challenges, developers and practitioners can fine-tune LLMs more effectively, achieving better performance and accuracy in their AI applications. Prompt Engineering for Fine-Tuning Prompt engineering is a technique that allows developers to customize Large Language Models (LLMs) for specific tasks without extensive fine-tuning. By\\n\\nSource: https://llmmodels.org/blog/llm-fine-tuning-guide-to-hitl-and-best-practices/\\nTitle: LLM Fine-Tuning: Guide to HITL & Best Practices\\nContent: Fine-tuning LLMs is beneficial in the following scenarios:\\nWhat is an Example of Human-in-the-Loop?\\nHuman-in-the-loop (HITL) fine-tuning involves human feedback and corrections to an LLM's outputs. For example:\\nIn the medical domain, medical professionals provide feedback on an LLM's diagnoses or treatment recommendations.\\nIn content moderation, human reviewers provide feedback on an LLM's generated text, flagging inappropriate or harmful content.\\nThis human input is used to fine-tune the model, enabling it to learn from expert knowledge and improve its performance.\\nUnsupervised Pre-training vs. Supervised Fine-tuning for LLMs\\nLLM Observability: Debugging with Arize Phoenix\\n10 Hyperparameter Tuning Tips for LLM Fine-Tuning\\nTop 10 Open-Source LLM Frameworks 2024\\nLLM Models and Directory Submissions: Ensuring Web Visibility Exploring Open Source AI Models: LLMs and Transformer Architectures Fairseq Tutorial: Getting Started with Sequence-to-Sequence Models\\n\\nSource: https://llmmodels.org/blog/llm-fine-tuning-guide-to-hitl-and-best-practices/\\nTitle: LLM Fine-Tuning: Guide to HITL & Best Practices\\nContent: of hyperparameters to find the best one Random search Randomly sample hyperparameters to find the best one Bayesian optimization Use Bayesian methods to find the optimal hyperparameters Model Selection and Modification Selecting the right pre-trained LLM and modifying it for the specific task at hand is crucial for fine-tuning. Model Selection and Modification Tips Tip Description Choose a relevant model Select a pre-trained model that aligns with the task requirements Modify the model architecture Adapt the model to incorporate task-specific knowledge Use transfer learning Leverage pre-trained models to fine-tune for the new task By following these best practices, developers can fine-tune LLMs that are accurate, reliable, and adaptable to specific tasks. Remember, fine-tuning is an iterative process that requires continuous refinement and evaluation to achieve optimal results. sbb-itb-f3e41df Using Human Feedback for Fine-Tuning Fine-tuning Large Language Models (LLMs) with human\\n\\nSource: https://llmmodels.org/blog/llm-fine-tuning-guide-to-hitl-and-best-practices/\\nTitle: LLM Fine-Tuning: Guide to HITL & Best Practices\\nContent: prevent overfitting. Early stopping Stops the training process when the model's performance on the validation set starts to degrade. Hyperparameter optimization Tunes the model's hyperparameters to find the optimal combination that prevents overfitting. Computational Resource Management Fine-tuning LLMs can be computationally expensive, requiring significant resources and infrastructure. This can be a challenge, especially for organizations with limited resources. Alternative Description Model distillation Trains a smaller model to mimic the behavior of a larger, pre-trained model. Efficient architectures Designs models that are computationally efficient and require fewer resources. By understanding and addressing these challenges, developers and practitioners can fine-tune LLMs more effectively, achieving better performance and accuracy in their AI applications. Prompt Engineering for Fine-Tuning Prompt engineering is a technique that allows developers to customize Large Language\\n\", \"Source: https://www.akaike.ai/resources/all-you-need-to-know-about-llm-fine-tuning-part-2\\nTitle: All You Need to Know About LLM Fine-Tuning (Part 2) | Akaike Ai\\nContent: to ensure that your fine-tuning efforts are not only successful but also sustainable in the long run. Whether you√¢\\x80\\x99re a seasoned machine learning practitioner or just beginning your journey, this article aims to provide you with actionable insights to master the art of fine-tuning LLMs.Popular Methods for LLM Fine-TuningFine-tuning LLMs involves tailoring a general model to specific tasks, and selecting the right method is essential for optimal performance. Methods like transfer learning adapt a broad model to related tasks (e.g., using a general language model for sentiment analysis of reviews), while task-specific fine-tuning focuses on training a model for particular applications (e.g., customizing a model for legal documents or medical terminology). Choosing the right method ensures the model performs well for its intended application. For example, fine-tuning with conversational data can improve a chatbot's understanding of customer queries, while task-specific fine-tuning might\\n\\nSource: https://www.akaike.ai/resources/all-you-need-to-know-about-llm-fine-tuning-part-2\\nTitle: All You Need to Know About LLM Fine-Tuning (Part 2) | Akaike Ai\\nContent: to ensure that your fine-tuning efforts are not only successful but also sustainable in the long run. Whether you√¢\\x80\\x99re a seasoned machine learning practitioner or just beginning your journey, this article aims to provide you with actionable insights to master the art of fine-tuning LLMs.Popular Methods for LLM Fine-TuningFine-tuning LLMs involves tailoring a general model to specific tasks, and selecting the right method is essential for optimal performance. Methods like transfer learning adapt a broad model to related tasks (e.g., using a general language model for sentiment analysis of reviews), while task-specific fine-tuning focuses on training a model for particular applications (e.g., customizing a model for legal documents or medical terminology). Choosing the right method ensures the model performs well for its intended application. For example, fine-tuning with conversational data can improve a chatbot's understanding of customer queries, while task-specific fine-tuning might\\n\\nSource: https://www.akaike.ai/resources/all-you-need-to-know-about-llm-fine-tuning-part-2\\nTitle: All You Need to Know About LLM Fine-Tuning (Part 2) | Akaike Ai\\nContent: to ensure that your fine-tuning efforts are not only successful but also sustainable in the long run. Whether you√¢\\x80\\x99re a seasoned machine learning practitioner or just beginning your journey, this article aims to provide you with actionable insights to master the art of fine-tuning LLMs.Popular Methods for LLM Fine-TuningFine-tuning LLMs involves tailoring a general model to specific tasks, and selecting the right method is essential for optimal performance. Methods like transfer learning adapt a broad model to related tasks (e.g., using a general language model for sentiment analysis of reviews), while task-specific fine-tuning focuses on training a model for particular applications (e.g., customizing a model for legal documents or medical terminology). Choosing the right method ensures the model performs well for its intended application. For example, fine-tuning with conversational data can improve a chatbot's understanding of customer queries, while task-specific fine-tuning might\\n\\nSource: https://www.akaike.ai/resources/all-you-need-to-know-about-llm-fine-tuning-part-2\\nTitle: All You Need to Know About LLM Fine-Tuning (Part 2) | Akaike Ai\\nContent: to ensure that your fine-tuning efforts are not only successful but also sustainable in the long run. Whether you√¢\\x80\\x99re a seasoned machine learning practitioner or just beginning your journey, this article aims to provide you with actionable insights to master the art of fine-tuning LLMs.Popular Methods for LLM Fine-TuningFine-tuning LLMs involves tailoring a general model to specific tasks, and selecting the right method is essential for optimal performance. Methods like transfer learning adapt a broad model to related tasks (e.g., using a general language model for sentiment analysis of reviews), while task-specific fine-tuning focuses on training a model for particular applications (e.g., customizing a model for legal documents or medical terminology). Choosing the right method ensures the model performs well for its intended application. For example, fine-tuning with conversational data can improve a chatbot's understanding of customer queries, while task-specific fine-tuning might\\n\\nSource: https://www.akaike.ai/resources/all-you-need-to-know-about-llm-fine-tuning-part-2\\nTitle: All You Need to Know About LLM Fine-Tuning (Part 2) | Akaike Ai\\nContent: to ensure that your fine-tuning efforts are not only successful but also sustainable in the long run. Whether you√¢\\x80\\x99re a seasoned machine learning practitioner or just beginning your journey, this article aims to provide you with actionable insights to master the art of fine-tuning LLMs.Popular Methods for LLM Fine-TuningFine-tuning LLMs involves tailoring a general model to specific tasks, and selecting the right method is essential for optimal performance. Methods like transfer learning adapt a broad model to related tasks (e.g., using a general language model for sentiment analysis of reviews), while task-specific fine-tuning focuses on training a model for particular applications (e.g., customizing a model for legal documents or medical terminology). Choosing the right method ensures the model performs well for its intended application. For example, fine-tuning with conversational data can improve a chatbot's understanding of customer queries, while task-specific fine-tuning might\\n\\nSource: https://www.akaike.ai/resources/all-you-need-to-know-about-llm-fine-tuning-part-2\\nTitle: All You Need to Know About LLM Fine-Tuning (Part 2) | Akaike Ai\\nContent: practices and strategic approaches to ensure that your fine-tuning efforts are not only successful but also sustainable in the long run. Whether you√¢\\x80\\x99re a seasoned machine learning practitioner or just beginning your journey, this article aims to provide you with actionable insights to master the art of fine-tuning LLMs.Popular Methods for LLM Fine-TuningFine-tuning LLMs involves tailoring a general model to specific tasks, and selecting the right method is essential for optimal performance. Methods like transfer learning adapt a broad model to related tasks (e.g., using a general language model for sentiment analysis of reviews), while task-specific fine-tuning focuses on training a model for particular applications (e.g., customizing a model for legal documents or medical terminology). Choosing the right method ensures the model performs well for its intended application. For example, fine-tuning with conversational data can improve a chatbot's understanding of customer queries,\\n\\nSource: https://www.akaike.ai/resources/all-you-need-to-know-about-llm-fine-tuning-part-2\\nTitle: All You Need to Know About LLM Fine-Tuning (Part 2) | Akaike Ai\\nContent: truly remarkable, specialized models? This blog will guide you through methods, best practices and strategic approaches to ensure that your fine-tuning efforts are not only successful but also sustainable in the long run. Whether you√¢\\x80\\x99re a seasoned machine learning practitioner or just beginning your journey, this article aims to provide you with actionable insights to master the art of fine-tuning LLMs.Popular Methods for LLM Fine-TuningFine-tuning LLMs involves tailoring a general model to specific tasks, and selecting the right method is essential for optimal performance. Methods like transfer learning adapt a broad model to related tasks (e.g., using a general language model for sentiment analysis of reviews), while task-specific fine-tuning focuses on training a model for particular applications (e.g., customizing a model for legal documents or medical terminology). Choosing the right method ensures the model performs well for its intended application. For example, fine-tuning\\n\\nSource: https://labelyourdata.com/articles/llm-fine-tuning\\nTitle: LLM Fine Tuning: The 2024 Guide for ML Teams | Label Your Data\\nContent: LLM fine-tuning is evolving with new techniques that make models more adaptable and efficient. Key developments include zero-shot and few-shot learning, which allow models to perform tasks with little to no task-specific data. These techniques enhance model generalization, but challenges like overfitting and the need for robust evaluation frameworks remain.Zero-Shot and Few-Shot Fine-TuningZero-shot and few-shot fine-tuning enable LLMs to handle tasks with minimal data. While zero-shot learning allows models to perform without prior examples, few-shot learning requires just a handful. Despite their potential, these methods face challenges like overfitting.Prompt Tuning and Hybrid ApproachesPrompt tuning, which refines the input prompts to improve model responses, is increasingly combined with traditional fine-tuning for enhanced performance. Hybrid approaches, blending prompt tuning with parameter-efficient fine-tuning methods, offer a balance between performance and computational\\n\\nSource: https://labelyourdata.com/articles/llm-fine-tuning\\nTitle: LLM Fine Tuning: The 2024 Guide for ML Teams | Label Your Data\\nContent: LLM fine-tuning is evolving with new techniques that make models more adaptable and efficient. Key developments include zero-shot and few-shot learning, which allow models to perform tasks with little to no task-specific data. These techniques enhance model generalization, but challenges like overfitting and the need for robust evaluation frameworks remain.Zero-Shot and Few-Shot Fine-TuningZero-shot and few-shot fine-tuning enable LLMs to handle tasks with minimal data. While zero-shot learning allows models to perform without prior examples, few-shot learning requires just a handful. Despite their potential, these methods face challenges like overfitting.Prompt Tuning and Hybrid ApproachesPrompt tuning, which refines the input prompts to improve model responses, is increasingly combined with traditional fine-tuning for enhanced performance. Hybrid approaches, blending prompt tuning with parameter-efficient fine-tuning methods, offer a balance between performance and computational\\n\\nSource: https://labelyourdata.com/articles/llm-fine-tuning\\nTitle: LLM Fine Tuning: The 2024 Guide for ML Teams | Label Your Data\\nContent: LLM fine-tuning is evolving with new techniques that make models more adaptable and efficient. Key developments include zero-shot and few-shot learning, which allow models to perform tasks with little to no task-specific data. These techniques enhance model generalization, but challenges like overfitting and the need for robust evaluation frameworks remain.Zero-Shot and Few-Shot Fine-TuningZero-shot and few-shot fine-tuning enable LLMs to handle tasks with minimal data. While zero-shot learning allows models to perform without prior examples, few-shot learning requires just a handful. Despite their potential, these methods face challenges like overfitting.Prompt Tuning and Hybrid ApproachesPrompt tuning, which refines the input prompts to improve model responses, is increasingly combined with traditional fine-tuning for enhanced performance. Hybrid approaches, blending prompt tuning with parameter-efficient fine-tuning methods, offer a balance between performance and computational\\n\", 'Source: https://www.ibm.com/think/topics/parameter-efficient-fine-tuning\\nTitle: What is parameter-efficient fine-tuning (PEFT)?\\nContent: Parameter-efficient fine-tuning brings a wealth of benefits that have made it popular with organizations that use LLMs in their work:\\nParameter-efficient fine-tuning brings a wealth of benefits that have made it popular with organizations that use LLMs in their work:\\nParameter-efficient fine-tuning brings a wealth of benefits that have made it popular with organizations that use LLMs in their work:\\nIncreased efficiency Most large language models used in generative AI (gen AI) are powered by expensive graphics processing units (GPUs) made by manufacturers such as Nvidia. Each LLM uses large amounts of computational resources and energy. Adjusting only the most relevant parameters imparts large savings on energy and cloud computing costs.\\n\\nSource: https://www.ibm.com/think/topics/parameter-efficient-fine-tuning\\nTitle: What is parameter-efficient fine-tuning (PEFT)?\\nContent: Parameter-efficient fine-tuning brings a wealth of benefits that have made it popular with organizations that use LLMs in their work: Increased efficiency Most large language models used in generative AI (gen AI) are powered by expensive graphics processing units (GPUs) made by manufacturers such as Nvidia. Each LLM uses large amounts of computational resources and energy. Adjusting only the most relevant parameters imparts large savings on energy and cloud computing costs. Faster time-to-value Time-to-value is the amount of time that it takes to develop, train and deploy an LLM so it can begin generating value for the organization that uses it. Because PEFT tweaks only a few trainable parameters, it takes far less time to update a model for a new task. PEFT can deliver comparable performance to a full fine-tuning process at a fraction of the time and expense. No catastrophic forgetting Catastrophic forgetting happens when LLMs lose or ‚Äúforget‚Äù the knowledge gained during the initial\\n\\nSource: https://www.analyticsvidhya.com/blog/2023/10/llm-fine-tuning-with-peft-techniques/\\nTitle: The Ultimate Guide to Parameter-efficient Fine-tuning with PEFT\\nContent: efficiency with acceptable error rates. Training Arguments: Define training arguments such as batch size, optimizer, learning rate scheduler, and checkpoints for your fine-tuning process. Fine-Tuning: Use the HuggingFace Trainer with your PEFT configuration to fine-tune your LLM. Monitor training progress using libraries like WandB. Validation: Keep an eye on both training and validation loss to ensure your model doesn‚Äôt overfit. Checkpointing: Save checkpoints to resume training from specific points if needed. Remember that fine-tuning an LLM, especially with PEFT, is a delicate balance between efficient parameter modification and maintaining model performance. Language Models and Fine-Tuning are powerful tools in the field of natural language processing. The PEFT technique, coupled with parameter efficiency strategies like LoRA and Quantization, allows us to make the most of these models efficiently. With the right configuration and careful training, we can unlock the true potential\\n\\nSource: https://www.analyticsvidhya.com/blog/2023/10/llm-fine-tuning-with-peft-techniques/\\nTitle: The Ultimate Guide to Parameter-efficient Fine-tuning with PEFT\\nContent: for Overfitting Remember, overfitting is a common issue during fine-tuning. To detect it, you need to track both training loss and validation loss. If the training loss keeps decreasing while the validation loss starts increasing, it‚Äôs a sign of overfitting. Ensure you have a separate validation dataset and pass it to the Trainer to monitor validation loss. That‚Äôs it! You‚Äôve successfully set up your environment and coded the fine-tuning process for your LLM using the PEFT technique. By following this step-by-step guide and monitoring your model‚Äôs performance, you‚Äôll be well on your way to leveraging the power of LLMs for various natural language understanding tasks. Conclusion In this exploration of language models and fine-tuning, we‚Äôve delved into the intricacies of harnessing the potential of LLMs through the innovative PEFT technique. This transformative approach allows us to efficiently adapt large models like Falcon 7B for specific tasks while balancing computational resources.\\n\\nSource: https://www.analyticsvidhya.com/blog/2023/10/llm-fine-tuning-with-peft-techniques/\\nTitle: The Ultimate Guide to Parameter-efficient Fine-tuning with PEFT\\nContent: for Overfitting Remember, overfitting is a common issue during fine-tuning. To detect it, you need to track both training loss and validation loss. If the training loss keeps decreasing while the validation loss starts increasing, it‚Äôs a sign of overfitting. Ensure you have a separate validation dataset and pass it to the Trainer to monitor validation loss. That‚Äôs it! You‚Äôve successfully set up your environment and coded the fine-tuning process for your LLM using the PEFT technique. By following this step-by-step guide and monitoring your model‚Äôs performance, you‚Äôll be well on your way to leveraging the power of LLMs for various natural language understanding tasks. Conclusion In this exploration of language models and fine-tuning, we‚Äôve delved into the intricacies of harnessing the potential of LLMs through the innovative PEFT technique. This transformative approach allows us to efficiently adapt large models like Falcon 7B for specific tasks while balancing computational resources.\\n\\nSource: https://medium.com/@shivansh.kaushik/efficient-model-fine-tuning-for-llms-understanding-peft-by-implementation-fc4d5e985389\\nTitle: Efficient Model Fine-Tuning for LLMs: Understanding PEFT by Implementation | by Shivansh Kaushik | Medium\\nContent: Efficient Model Fine-Tuning for LLMs: Understanding PEFT by ImplementationShivansh Kaushik¬∑Follow7 min read¬∑Jul 31, 2023--ListenShareFine-tuning large language models (LLMs) is a computationally intensive process, posing challenges in terms of memory requirements and time consumption. To address these issues, researchers have developed Parameter Efficient Fine-Tuning (PEFT) methods, which optimize fine-tuning by updating only a small subset of model parameters. In this article, we explore the key concepts of PEFT and two prominent techniques, LoRA and Prompt Tuning, that offer efficient adaptation of LLMs for specific tasks.The Challenges of Full Fine-TuningLarge language models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks, but their success comes at a cost ‚Äî computational intensity. Full fine-tuning, the traditional method of adapting these models to specific tasks, requires updating all model weights during supervised learning. While this\\n\\nSource: https://medium.com/@shivansh.kaushik/efficient-model-fine-tuning-for-llms-understanding-peft-by-implementation-fc4d5e985389\\nTitle: Efficient Model Fine-Tuning for LLMs: Understanding PEFT by Implementation | by Shivansh Kaushik | Medium\\nContent: Efficient Model Fine-Tuning for LLMs: Understanding PEFT by ImplementationShivansh Kaushik¬∑Follow7 min read¬∑Jul 31, 2023--ListenShareFine-tuning large language models (LLMs) is a computationally intensive process, posing challenges in terms of memory requirements and time consumption. To address these issues, researchers have developed Parameter Efficient Fine-Tuning (PEFT) methods, which optimize fine-tuning by updating only a small subset of model parameters. In this article, we explore the key concepts of PEFT and two prominent techniques, LoRA and Prompt Tuning, that offer efficient adaptation of LLMs for specific tasks.The Challenges of Full Fine-TuningLarge language models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks, but their success comes at a cost ‚Äî computational intensity. Full fine-tuning, the traditional method of adapting these models to specific tasks, requires updating all model weights during supervised learning. While this\\n\\nSource: https://medium.com/@shivansh.kaushik/efficient-model-fine-tuning-for-llms-understanding-peft-by-implementation-fc4d5e985389\\nTitle: Efficient Model Fine-Tuning for LLMs: Understanding PEFT by Implementation | by Shivansh Kaushik | Medium\\nContent: Efficient Model Fine-Tuning for LLMs: Understanding PEFT by ImplementationShivansh Kaushik¬∑Follow7 min read¬∑Jul 31, 2023--ListenShareFine-tuning large language models (LLMs) is a computationally intensive process, posing challenges in terms of memory requirements and time consumption. To address these issues, researchers have developed Parameter Efficient Fine-Tuning (PEFT) methods, which optimize fine-tuning by updating only a small subset of model parameters. In this article, we explore the key concepts of PEFT and two prominent techniques, LoRA and Prompt Tuning, that offer efficient adaptation of LLMs for specific tasks.The Challenges of Full Fine-TuningLarge language models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks, but their success comes at a cost ‚Äî computational intensity. Full fine-tuning, the traditional method of adapting these models to specific tasks, requires updating all model weights during supervised learning. While this\\n\\nSource: https://medium.com/@shivansh.kaushik/efficient-model-fine-tuning-for-llms-understanding-peft-by-implementation-fc4d5e985389\\nTitle: Efficient Model Fine-Tuning for LLMs: Understanding PEFT by Implementation | by Shivansh Kaushik | Medium\\nContent: Efficient Model Fine-Tuning for LLMs: Understanding PEFT by ImplementationShivansh Kaushik¬∑Follow7 min read¬∑Jul 31, 2023--ListenShareFine-tuning large language models (LLMs) is a computationally intensive process, posing challenges in terms of memory requirements and time consumption. To address these issues, researchers have developed Parameter Efficient Fine-Tuning (PEFT) methods, which optimize fine-tuning by updating only a small subset of model parameters. In this article, we explore the key concepts of PEFT and two prominent techniques, LoRA and Prompt Tuning, that offer efficient adaptation of LLMs for specific tasks.The Challenges of Full Fine-TuningLarge language models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks, but their success comes at a cost ‚Äî computational intensity. Full fine-tuning, the traditional method of adapting these models to specific tasks, requires updating all model weights during supervised learning. While this\\n\\nSource: https://medium.com/@shivansh.kaushik/efficient-model-fine-tuning-for-llms-understanding-peft-by-implementation-fc4d5e985389\\nTitle: Efficient Model Fine-Tuning for LLMs: Understanding PEFT by Implementation | by Shivansh Kaushik | Medium\\nContent: Efficient Model Fine-Tuning for LLMs: Understanding PEFT by ImplementationShivansh Kaushik¬∑Follow7 min read¬∑Jul 31, 2023--ListenShareFine-tuning large language models (LLMs) is a computationally intensive process, posing challenges in terms of memory requirements and time consumption. To address these issues, researchers have developed Parameter Efficient Fine-Tuning (PEFT) methods, which optimize fine-tuning by updating only a small subset of model parameters. In this article, we explore the key concepts of PEFT and two prominent techniques, LoRA and Prompt Tuning, that offer efficient adaptation of LLMs for specific tasks.The Challenges of Full Fine-TuningLarge language models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks, but their success comes at a cost ‚Äî computational intensity. Full fine-tuning, the traditional method of adapting these models to specific tasks, requires updating all model weights during supervised learning. While this\\n', 'Source: https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora\\nTitle: In-depth guide to fine-tuning LLMs with LoRA and QLoRA\\nContent: for QLoRA training.QLoRA vs Standard FinetuningIn the paper, researchers provide a very detailed comparison between QLoRA, LoRA, and Full Finetuning of a network.√¢\\x80\\x8dAs you can see in the above table, there is no loss of performance whatsoever in the T5 model family upon training with QLoRA and even with Double Quantization, we don√¢\\x80\\x99t see any major differences. One significant difference is the number of LoRA adapters required. In the paper, the authors mention that they needed more LoRA adapters for QLoRA finetuning, compared to normal LoRA finetuning. The authors suggest applying the LoRA adapters on all the linear transformer blocks along with the query, key, and value layers.√¢\\x80\\x8dEven for the much bigger language models, performance remains the same:√¢\\x80\\x8dHence the authors trained a model on top of the base LLaMA models, Guanaco. This is a 33 billion parameter model, trained on the OASST1 dataset. At the time of its release, it became a state-of-the-art model and achieved 99.3%\\n\\nSource: https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora\\nTitle: In-depth guide to fine-tuning LLMs with LoRA and QLoRA\\nContent: for QLoRA training.QLoRA vs Standard FinetuningIn the paper, researchers provide a very detailed comparison between QLoRA, LoRA, and Full Finetuning of a network.√¢\\x80\\x8dAs you can see in the above table, there is no loss of performance whatsoever in the T5 model family upon training with QLoRA and even with Double Quantization, we don√¢\\x80\\x99t see any major differences. One significant difference is the number of LoRA adapters required. In the paper, the authors mention that they needed more LoRA adapters for QLoRA finetuning, compared to normal LoRA finetuning. The authors suggest applying the LoRA adapters on all the linear transformer blocks along with the query, key, and value layers.√¢\\x80\\x8dEven for the much bigger language models, performance remains the same:√¢\\x80\\x8dHence the authors trained a model on top of the base LLaMA models, Guanaco. This is a 33 billion parameter model, trained on the OASST1 dataset. At the time of its release, it became a state-of-the-art model and achieved 99.3%\\n\\nSource: https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora\\nTitle: In-depth guide to fine-tuning LLMs with LoRA and QLoRA\\nContent: for QLoRA training.QLoRA vs Standard FinetuningIn the paper, researchers provide a very detailed comparison between QLoRA, LoRA, and Full Finetuning of a network.√¢\\x80\\x8dAs you can see in the above table, there is no loss of performance whatsoever in the T5 model family upon training with QLoRA and even with Double Quantization, we don√¢\\x80\\x99t see any major differences. One significant difference is the number of LoRA adapters required. In the paper, the authors mention that they needed more LoRA adapters for QLoRA finetuning, compared to normal LoRA finetuning. The authors suggest applying the LoRA adapters on all the linear transformer blocks along with the query, key, and value layers.√¢\\x80\\x8dEven for the much bigger language models, performance remains the same:√¢\\x80\\x8dHence the authors trained a model on top of the base LLaMA models, Guanaco. This is a 33 billion parameter model, trained on the OASST1 dataset. At the time of its release, it became a state-of-the-art model and achieved 99.3%\\n\\nSource: https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora\\nTitle: In-depth guide to fine-tuning LLMs with LoRA and QLoRA\\nContent: for QLoRA training.QLoRA vs Standard FinetuningIn the paper, researchers provide a very detailed comparison between QLoRA, LoRA, and Full Finetuning of a network.√¢\\x80\\x8dAs you can see in the above table, there is no loss of performance whatsoever in the T5 model family upon training with QLoRA and even with Double Quantization, we don√¢\\x80\\x99t see any major differences. One significant difference is the number of LoRA adapters required. In the paper, the authors mention that they needed more LoRA adapters for QLoRA finetuning, compared to normal LoRA finetuning. The authors suggest applying the LoRA adapters on all the linear transformer blocks along with the query, key, and value layers.√¢\\x80\\x8dEven for the much bigger language models, performance remains the same:√¢\\x80\\x8dHence the authors trained a model on top of the base LLaMA models, Guanaco. This is a 33 billion parameter model, trained on the OASST1 dataset. At the time of its release, it became a state-of-the-art model and achieved 99.3%\\n\\nSource: https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora\\nTitle: In-depth guide to fine-tuning LLMs with LoRA and QLoRA\\nContent: for QLoRA training.QLoRA vs Standard FinetuningIn the paper, researchers provide a very detailed comparison between QLoRA, LoRA, and Full Finetuning of a network.√¢\\x80\\x8dAs you can see in the above table, there is no loss of performance whatsoever in the T5 model family upon training with QLoRA and even with Double Quantization, we don√¢\\x80\\x99t see any major differences. One significant difference is the number of LoRA adapters required. In the paper, the authors mention that they needed more LoRA adapters for QLoRA finetuning, compared to normal LoRA finetuning. The authors suggest applying the LoRA adapters on all the linear transformer blocks along with the query, key, and value layers.√¢\\x80\\x8dEven for the much bigger language models, performance remains the same:√¢\\x80\\x8dHence the authors trained a model on top of the base LLaMA models, Guanaco. This is a 33 billion parameter model, trained on the OASST1 dataset. At the time of its release, it became a state-of-the-art model and achieved 99.3%\\n\\nSource: https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora\\nTitle: In-depth guide to fine-tuning LLMs with LoRA and QLoRA\\nContent: QLoRA vs Standard Finetuning\\nIn the paper, researchers provide a very detailed comparison between QLoRA, LoRA, and Full Finetuning of a network.\\nAs you can see in the above table, there is no loss of performance whatsoever in the T5 model family upon training with QLoRA and even with Double Quantization, we don√¢\\x80\\x99t see any major differences. One significant difference is the number of LoRA adapters required. In the paper, the authors mention that they needed more LoRA adapters for QLoRA finetuning, compared to normal LoRA finetuning. The authors suggest applying the LoRA adapters on all the linear transformer blocks along with the query, key, and value layers.\\nEven for the much bigger language models, performance remains the same:\\n\\nSource: https://blog.lancedb.com/optimizing-llms-a-step-by-step-guide-to-fine-tuning-with-peft-and-qlora-22eddd13d25b/\\nTitle: Optimizing LLMs: A Step-by-Step Guide to Fine-Tuning with PEFT and QLoRA\\nContent: Performance: LoRA‚Äôs efficiency doesn‚Äôt come at the cost of performance. While the reduction in parameters might lead to slightly lower performance gains compared to full fine-tuning, LoRA still delivers impressive results, especially when compared to the base LLM model.In practice, LoRA is an invaluable tool for efficiently fine-tuning LLMs and adapting them to specific tasks without overwhelming computational and memory resources. It strikes a balance between parameter efficiency and performance, making it a go-to technique for many natural language processing applications.But wait, there‚Äôs a game-changer on the horizon ‚Äî QLoRA.What Is QLoRA?QLoRA, which stands for Quantized Low-rank Adaptation, takes fine-tuning to the next level. It empowers you to fine-tune LLMs on a single GPU, pushing the boundaries of what‚Äôs possible. How does QLoRA differ from LoRA?The paper introduces QLoRA, an efficient fine-tuning method that enables the training of a 65-billion-parameter language model on\\n\\nSource: https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora\\nTitle: In-depth guide to fine-tuning LLMs with LoRA and QLoRA\\nContent: have lesser models like Vicuna 13B, and Guanaco33B because of the use of 4-bit precision used less memory than the 13B model.√¢\\x80\\x8dOther variants of LoRA finetuningQA LoRAQA LoRA is another fine-tuning technique built on top of QLoRA, introduced in this paper. QALoRA was mainly released for finetuning diffusion models, but can easily be generalized for training any type of models, just like LoRA.√¢\\x80\\x8dThe difference between QLoRA and QALoRA is that QALoRA is quantization aware meaning the weights of the LoRA adapters are also quantized along with the weights of the model during the finetuning process. This helps in more efficient training as there is no need for the conversion step to update the models during the backpropagation process.√¢\\x80\\x8dLongLoRA√¢\\x80\\x8dLongLoRA is yet another variation of the LoRA finetuning technique, but this technique is specifically for training longer context models. LongLoRA works by using something called SHIFT SHORT ATTENTION. This method creates chunks or groups of\\n\\nSource: https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora\\nTitle: In-depth guide to fine-tuning LLMs with LoRA and QLoRA\\nContent: have lesser models like Vicuna 13B, and Guanaco33B because of the use of 4-bit precision used less memory than the 13B model.√¢\\x80\\x8dOther variants of LoRA finetuningQA LoRAQA LoRA is another fine-tuning technique built on top of QLoRA, introduced in this paper. QALoRA was mainly released for finetuning diffusion models, but can easily be generalized for training any type of models, just like LoRA.√¢\\x80\\x8dThe difference between QLoRA and QALoRA is that QALoRA is quantization aware meaning the weights of the LoRA adapters are also quantized along with the weights of the model during the finetuning process. This helps in more efficient training as there is no need for the conversion step to update the models during the backpropagation process.√¢\\x80\\x8dLongLoRA√¢\\x80\\x8dLongLoRA is yet another variation of the LoRA finetuning technique, but this technique is specifically for training longer context models. LongLoRA works by using something called SHIFT SHORT ATTENTION. This method creates chunks or groups of\\n\\nSource: https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora\\nTitle: In-depth guide to fine-tuning LLMs with LoRA and QLoRA\\nContent: have lesser models like Vicuna 13B, and Guanaco33B because of the use of 4-bit precision used less memory than the 13B model.√¢\\x80\\x8dOther variants of LoRA finetuningQA LoRAQA LoRA is another fine-tuning technique built on top of QLoRA, introduced in this paper. QALoRA was mainly released for finetuning diffusion models, but can easily be generalized for training any type of models, just like LoRA.√¢\\x80\\x8dThe difference between QLoRA and QALoRA is that QALoRA is quantization aware meaning the weights of the LoRA adapters are also quantized along with the weights of the model during the finetuning process. This helps in more efficient training as there is no need for the conversion step to update the models during the backpropagation process.√¢\\x80\\x8dLongLoRA√¢\\x80\\x8dLongLoRA is yet another variation of the LoRA finetuning technique, but this technique is specifically for training longer context models. LongLoRA works by using something called SHIFT SHORT ATTENTION. This method creates chunks or groups of\\n', \"Source: https://www.lakera.ai/blog/llm-fine-tuning-guide\\nTitle: The Ultimate Guide to LLM Fine Tuning: Best Practices & Tools | Lakera √¢\\x80\\x93 Protecting AI teams that disrupt the world.\\nContent: Language Models Fine Tuning MethodsLarge Language Models (LLMs) Fine Tuning Methods encompass a variety of techniques, ranging from traditional, time-tested approaches to innovative, cutting-edge strategies, all aimed at enhancing the performance and applicability of these powerful models in diverse contexts.Early Fine-tuning Methods / Old-schoolIn old-school approaches, there are various methods to fine tune pre-trained language models, each tailored to specific needs and resource constraints.Feature-based: It uses a pre-trained LLM as a feature extractor, transforming input text into a fixed-sized array. A separate classifier network predicts the text's class probability in NLP tasks. In training, only the classifier's weights change, making it resource-friendly but potentially less performant.Finetuning I: Finetuning I enhances the pre-trained LLM by adding extra dense layers. During training, only the newly added layers' weights are adjusted while keeping the pre-trained LLM\\n\\nSource: https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07\\nTitle: Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA | by Suman Das | Medium\\nContent: the model to the nuances of the target domain.Fine-tuning LLMs is commonly used in natural language processing tasks such as sentiment analysis, named entity recognition, summarization, translation, or any other application where understanding context and generating coherent language is crucial. It helps leverage the knowledge encoded in pre-trained models for more specialized and domain-specific tasks.Fine-tuning methodsFine-tuning a Large Language Model (LLM) involves a supervised learning process. In this method, a dataset comprising labeled examples is utilized to adjust the model‚Äôs weights, enhancing its proficiency in specific tasks. Now, let‚Äôs delve into some noteworthy techniques employed in the fine-tuning process.Full Fine Tuning (Instruction fine-tuning): Instruction fine-tuning is a strategy to enhance a model‚Äôs performance across various tasks by training it on examples that guide its responses to queries. The choice of the dataset is crucial and tailored to the specific\\n\\nSource: https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07\\nTitle: Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA | by Suman Das | Medium\\nContent: the model to the nuances of the target domain.Fine-tuning LLMs is commonly used in natural language processing tasks such as sentiment analysis, named entity recognition, summarization, translation, or any other application where understanding context and generating coherent language is crucial. It helps leverage the knowledge encoded in pre-trained models for more specialized and domain-specific tasks.Fine-tuning methodsFine-tuning a Large Language Model (LLM) involves a supervised learning process. In this method, a dataset comprising labeled examples is utilized to adjust the model‚Äôs weights, enhancing its proficiency in specific tasks. Now, let‚Äôs delve into some noteworthy techniques employed in the fine-tuning process.Full Fine Tuning (Instruction fine-tuning): Instruction fine-tuning is a strategy to enhance a model‚Äôs performance across various tasks by training it on examples that guide its responses to queries. The choice of the dataset is crucial and tailored to the specific\\n\\nSource: https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07\\nTitle: Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA | by Suman Das | Medium\\nContent: the model to the nuances of the target domain.Fine-tuning LLMs is commonly used in natural language processing tasks such as sentiment analysis, named entity recognition, summarization, translation, or any other application where understanding context and generating coherent language is crucial. It helps leverage the knowledge encoded in pre-trained models for more specialized and domain-specific tasks.Fine-tuning methodsFine-tuning a Large Language Model (LLM) involves a supervised learning process. In this method, a dataset comprising labeled examples is utilized to adjust the model‚Äôs weights, enhancing its proficiency in specific tasks. Now, let‚Äôs delve into some noteworthy techniques employed in the fine-tuning process.Full Fine Tuning (Instruction fine-tuning): Instruction fine-tuning is a strategy to enhance a model‚Äôs performance across various tasks by training it on examples that guide its responses to queries. The choice of the dataset is crucial and tailored to the specific\\n\\nSource: https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07\\nTitle: Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA | by Suman Das | Medium\\nContent: the model to the nuances of the target domain.Fine-tuning LLMs is commonly used in natural language processing tasks such as sentiment analysis, named entity recognition, summarization, translation, or any other application where understanding context and generating coherent language is crucial. It helps leverage the knowledge encoded in pre-trained models for more specialized and domain-specific tasks.Fine-tuning methodsFine-tuning a Large Language Model (LLM) involves a supervised learning process. In this method, a dataset comprising labeled examples is utilized to adjust the model‚Äôs weights, enhancing its proficiency in specific tasks. Now, let‚Äôs delve into some noteworthy techniques employed in the fine-tuning process.Full Fine Tuning (Instruction fine-tuning): Instruction fine-tuning is a strategy to enhance a model‚Äôs performance across various tasks by training it on examples that guide its responses to queries. The choice of the dataset is crucial and tailored to the specific\\n\\nSource: https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07\\nTitle: Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA | by Suman Das | Medium\\nContent: the model to the nuances of the target domain.Fine-tuning LLMs is commonly used in natural language processing tasks such as sentiment analysis, named entity recognition, summarization, translation, or any other application where understanding context and generating coherent language is crucial. It helps leverage the knowledge encoded in pre-trained models for more specialized and domain-specific tasks.Fine-tuning methodsFine-tuning a Large Language Model (LLM) involves a supervised learning process. In this method, a dataset comprising labeled examples is utilized to adjust the model‚Äôs weights, enhancing its proficiency in specific tasks. Now, let‚Äôs delve into some noteworthy techniques employed in the fine-tuning process.Full Fine Tuning (Instruction fine-tuning): Instruction fine-tuning is a strategy to enhance a model‚Äôs performance across various tasks by training it on examples that guide its responses to queries. The choice of the dataset is crucial and tailored to the specific\\n\\nSource: https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07\\nTitle: Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA | by Suman Das | Medium\\nContent: the model to the nuances of the target domain.Fine-tuning LLMs is commonly used in natural language processing tasks such as sentiment analysis, named entity recognition, summarization, translation, or any other application where understanding context and generating coherent language is crucial. It helps leverage the knowledge encoded in pre-trained models for more specialized and domain-specific tasks.Fine-tuning methodsFine-tuning a Large Language Model (LLM) involves a supervised learning process. In this method, a dataset comprising labeled examples is utilized to adjust the model‚Äôs weights, enhancing its proficiency in specific tasks. Now, let‚Äôs delve into some noteworthy techniques employed in the fine-tuning process.Full Fine Tuning (Instruction fine-tuning): Instruction fine-tuning is a strategy to enhance a model‚Äôs performance across various tasks by training it on examples that guide its responses to queries. The choice of the dataset is crucial and tailored to the specific\\n\\nSource: https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07\\nTitle: Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA | by Suman Das | Medium\\nContent: the model to the nuances of the target domain.Fine-tuning LLMs is commonly used in natural language processing tasks such as sentiment analysis, named entity recognition, summarization, translation, or any other application where understanding context and generating coherent language is crucial. It helps leverage the knowledge encoded in pre-trained models for more specialized and domain-specific tasks.Fine-tuning methodsFine-tuning a Large Language Model (LLM) involves a supervised learning process. In this method, a dataset comprising labeled examples is utilized to adjust the model‚Äôs weights, enhancing its proficiency in specific tasks. Now, let‚Äôs delve into some noteworthy techniques employed in the fine-tuning process.Full Fine Tuning (Instruction fine-tuning): Instruction fine-tuning is a strategy to enhance a model‚Äôs performance across various tasks by training it on examples that guide its responses to queries. The choice of the dataset is crucial and tailored to the specific\\n\\nSource: https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07\\nTitle: Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA | by Suman Das | Medium\\nContent: the model to the nuances of the target domain.Fine-tuning LLMs is commonly used in natural language processing tasks such as sentiment analysis, named entity recognition, summarization, translation, or any other application where understanding context and generating coherent language is crucial. It helps leverage the knowledge encoded in pre-trained models for more specialized and domain-specific tasks.Fine-tuning methodsFine-tuning a Large Language Model (LLM) involves a supervised learning process. In this method, a dataset comprising labeled examples is utilized to adjust the model‚Äôs weights, enhancing its proficiency in specific tasks. Now, let‚Äôs delve into some noteworthy techniques employed in the fine-tuning process.Full Fine Tuning (Instruction fine-tuning): Instruction fine-tuning is a strategy to enhance a model‚Äôs performance across various tasks by training it on examples that guide its responses to queries. The choice of the dataset is crucial and tailored to the specific\\n\\nSource: https://www.lakera.ai/blog/llm-fine-tuning-guide\\nTitle: The Ultimate Guide to LLM Fine Tuning: Best Practices & Tools | Lakera √¢\\x80\\x93 Protecting AI teams that disrupt the world.\\nContent: Challenges & limitations of LLM fine tuning\\nSome of the main challenges and limitations associated with fine-tuning LLMs:\\nOverfitting: Fine-tuning can be prone to overfitting, a condition where the model becomes overly specialized on the training data and performs poorly on unseen data. This risk is particularly pronounced when the task-specific dataset is small or not representative of the broader context.\\nCatastrophic Forgetting: During fine-tuning for a specific task, the model may forget previously acquired general knowledge. This phenomenon, known as catastrophic forgetting, can impair the model's adaptability to diverse tasks.\\nBias Amplification: Pre-trained models inherit biases from their training data, which fine-tuning can inadvertently amplify when applied to task-specific data. This amplification may lead to biased predictions and outputs, potentially causing ethical concerns.\\n\"]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from gpt_researcher import GPTResearcher\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "QUERY = \"What are the best ways to use a loss functions when fine tuning an LLM model?\"\n",
    "REPORT_TYPE = \"research_report\"\n",
    "async def main():\n",
    "    researcher = GPTResearcher(query=QUERY, report_type=REPORT_TYPE)\n",
    "    report = await researcher.conduct_research()\n",
    "    return report\n",
    "\n",
    "report = asyncio.run(main())\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     [20:48:33] üîç Starting the research task for 'What are the best ways to use a loss functions when fine tuning an LLM model?'...\n",
      "INFO:     [20:48:33] Default Agent\n",
      "INFO:     [20:48:33] üåê Browsing the web to learn more about the task: What are the best ways to use a loss functions when fine tuning an LLM model?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error in reading JSON, attempting to repair JSON\n",
      "Error decoding JSON: Invalid \\escape: line 3 column 7 (char 46)\n",
      "No JSON found in the string. Falling back to Default Agent.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     [20:48:36] ü§î Planning the research strategy and subtasks (this may take a minute)...\n",
      "INFO:     [20:48:36] üóÇÔ∏è I will conduct my research based on the following queries: ['fine tuning LLMs loss function best practices 2024', 'choosing loss function for LLM fine tuning', 'LLM fine tuning optimizers and loss functions', 'parameter efficient fine tuning loss functions', 'What are the best ways to use a loss functions when fine tuning an LLM model?']...\n",
      "INFO:     [20:48:36] \n",
      "üîç Running research for 'fine tuning LLMs loss function best practices 2024'...\n",
      "INFO:     [20:48:36] \n",
      "üîç Running research for 'choosing loss function for LLM fine tuning'...\n",
      "INFO:     [20:48:36] \n",
      "üîç Running research for 'LLM fine tuning optimizers and loss functions'...\n",
      "INFO:     [20:48:36] \n",
      "üîç Running research for 'parameter efficient fine tuning loss functions'...\n",
      "INFO:     [20:48:36] \n",
      "üîç Running research for 'What are the best ways to use a loss functions when fine tuning an LLM model?'...\n",
      "INFO:     [20:48:39] ‚úÖ Added source url to research: https://www.marketingscoop.com/ai/llm-fine-tuning/\n",
      "\n",
      "INFO:     [20:48:39] ‚úÖ Added source url to research: https://medium.com/ai-native-publication/fine-tuning-large-language-models-strategies-challenges-and-best-practices-5cbf6dd1b8de\n",
      "\n",
      "INFO:     [20:48:39] ‚úÖ Added source url to research: https://www.researchgate.net/publication/383428454_The_Ultimate_Guide_to_Fine-Tuning_LLMs_from_Basics_to_Breakthroughs_An_Exhaustive_Review_of_Technologies_Research_Best_Practices_Applied_Research_Challenges_and_Opportunities\n",
      "\n",
      "INFO:     [20:48:39] ‚úÖ Added source url to research: https://www.restack.io/p/fine-tuning-answer-llms-hugging-face-2024-cat-ai\n",
      "\n",
      "INFO:     [20:48:39] ‚úÖ Added source url to research: https://labelyourdata.com/articles/llm-fine-tuning\n",
      "\n",
      "INFO:     [20:48:39] ü§î Researching for relevant information across multiple sources...\n",
      "\n",
      "INFO:     [20:48:39] üåê Scraping content from 5 URLs...\n",
      "/opt/anaconda3/envs/algo/lib/python3.11/site-packages/bs4/__init__.py:618: RuntimeWarning: coroutine 'research_report' was never awaited\n",
      "  self.object_was_parsed(o)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "INFO:     [20:48:44] üìÑ Scraped 4 pages of content\n",
      "INFO:     [20:48:44] üñºÔ∏è Selected 1 new images from 3 total images\n",
      "INFO:     [20:48:44] üåê Scraping complete\n",
      "INFO:     [20:48:44] üìö Getting relevant content based on query: fine tuning LLMs loss function best practices 2024...\n",
      "INFO:     [20:48:44] ‚úÖ Added source url to research: https://markovate.com/blog/parameter-efficient-fine-tuning-peft-of-llms-a-practical-guide/\n",
      "\n",
      "INFO:     [20:48:44] ‚úÖ Added source url to research: https://medium.com/@techsachin/parameter-efficient-fine-tuning-for-models-categories-and-algorithms-4481fb2bdef0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error! : HTTPSConnectionPool(host='www.marketingscoop.com', port=443): Read timed out. (read timeout=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     [20:48:44] ‚úÖ Added source url to research: https://www.cs.cmu.edu/~mgormley/courses/10423/slides/lecture10-peft.pdf\n",
      "\n",
      "INFO:     [20:48:44] ‚úÖ Added source url to research: https://kanerika.com/blogs/parameter-efficient-fine-tuning/\n",
      "\n",
      "INFO:     [20:48:44] ‚úÖ Added source url to research: https://huggingface.co/blog/peft\n",
      "\n",
      "INFO:     [20:48:44] ü§î Researching for relevant information across multiple sources...\n",
      "\n",
      "INFO:     [20:48:44] üåê Scraping content from 5 URLs...\n",
      "INFO:     [20:49:09] üìÑ Scraped 4 pages of content\n",
      "INFO:     [20:49:09] üñºÔ∏è Selected 4 new images from 8 total images\n",
      "INFO:     [20:49:09] üåê Scraping complete\n",
      "INFO:     [20:49:09] üìö Getting relevant content based on query: parameter efficient fine tuning loss functions...\n",
      "INFO:     [20:49:09] ‚úÖ Added source url to research: https://arxiv.org/html/2408.13296v1\n",
      "\n",
      "INFO:     [20:49:09] ‚úÖ Added source url to research: https://www.geeksforgeeks.org/fine-tuning-large-language-model-llm/\n",
      "\n",
      "INFO:     [20:49:09] ‚úÖ Added source url to research: https://www.lakera.ai/blog/llm-fine-tuning-guide\n",
      "\n",
      "INFO:     [20:49:09] ‚úÖ Added source url to research: https://medium.com/@shivansh.kaushik/efficient-model-fine-tuning-for-llms-understanding-peft-by-implementation-fc4d5e985389\n",
      "\n",
      "INFO:     [20:49:09] ‚úÖ Added source url to research: https://www.turing.com/resources/finetuning-large-language-models\n",
      "\n",
      "INFO:     [20:49:09] ü§î Researching for relevant information across multiple sources...\n",
      "\n",
      "INFO:     [20:49:09] üåê Scraping content from 5 URLs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing dimension value 49.72: invalid literal for int() with base 10: '49.72'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     [20:49:10] üìÑ Scraped 4 pages of content\n",
      "INFO:     [20:49:10] üñºÔ∏è Selected 1 new images from 1 total images\n",
      "INFO:     [20:49:10] üåê Scraping complete\n",
      "INFO:     [20:49:10] üìö Getting relevant content based on query: What are the best ways to use a loss functions when fine tuning an LLM model?...\n",
      "INFO:     [20:49:10] ‚úÖ Added source url to research: https://medium.com/@codethulo/fine-tuning-llms-with-instruction-80752d31295d\n",
      "\n",
      "INFO:     [20:49:10] ‚úÖ Added source url to research: https://www.akaike.ai/resources/all-you-need-to-know-about-llm-fine-tuning-part-2\n",
      "\n",
      "INFO:     [20:49:10] ‚úÖ Added source url to research: https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning\n",
      "\n",
      "INFO:     [20:49:10] ‚úÖ Added source url to research: https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms\n",
      "\n",
      "INFO:     [20:49:10] ü§î Researching for relevant information across multiple sources...\n",
      "\n",
      "INFO:     [20:49:10] üåê Scraping content from 4 URLs...\n",
      "INFO:     [20:49:10] üìÑ Scraped 4 pages of content\n",
      "INFO:     [20:49:10] üñºÔ∏è Selected 4 new images from 10 total images\n",
      "INFO:     [20:49:10] üåê Scraping complete\n",
      "INFO:     [20:49:10] üìö Getting relevant content based on query: choosing loss function for LLM fine tuning...\n",
      "INFO:     [20:49:10] ‚úÖ Added source url to research: https://symbl.ai/developers/blog/guide-to-fine-tuning-techniques-for-llms/\n",
      "\n",
      "INFO:     [20:49:10] ‚úÖ Added source url to research: https://medium.com/ubiai-nlp/fine-tuning-llm-a-deep-dive-into-advanced-techniques-for-optimal-model-performance-289affdfaf61\n",
      "\n",
      "INFO:     [20:49:10] ‚úÖ Added source url to research: https://blog.lancedb.com/optimizing-llms-a-step-by-step-guide-to-fine-tuning-with-peft-and-qlora-22eddd13d25b/\n",
      "\n",
      "INFO:     [20:49:10] ü§î Researching for relevant information across multiple sources...\n",
      "\n",
      "INFO:     [20:49:10] üåê Scraping content from 3 URLs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing dimension value 330.75: invalid literal for int() with base 10: '330.75'\n",
      "Error parsing dimension value 289.6756238003839: invalid literal for int() with base 10: '289.6756238003839'\n",
      "Error parsing dimension value 257.11567164179104: invalid literal for int() with base 10: '257.11567164179104'\n",
      "Error parsing dimension value 250.3434065934066: invalid literal for int() with base 10: '250.3434065934066'\n",
      "Error parsing dimension value 229.9794921875: invalid literal for int() with base 10: '229.9794921875'\n",
      "Error parsing dimension value 404.25: invalid literal for int() with base 10: '404.25'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     [20:49:11] üìÑ Scraped 3 pages of content\n",
      "INFO:     [20:49:11] üñºÔ∏è Selected 1 new images from 1 total images\n",
      "INFO:     [20:49:11] üåê Scraping complete\n",
      "INFO:     [20:49:11] üìö Getting relevant content based on query: LLM fine tuning optimizers and loss functions...\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m report\n\u001b[1;32m     15\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the best ways to use a loss functions when fine tuning an LLM model?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m report \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m research_report(query)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(report)\n",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m, in \u001b[0;36mresearch_report\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresearch_report\u001b[39m(query):\n\u001b[1;32m      6\u001b[0m     researcher \u001b[38;5;241m=\u001b[39m GPTResearcher(query\u001b[38;5;241m=\u001b[39mquery, report_type\u001b[38;5;241m=\u001b[39mREPORT_TYPE)\n\u001b[0;32m----> 7\u001b[0m     report \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m researcher\u001b[38;5;241m.\u001b[39mconduct_research()\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m report\n",
      "File \u001b[0;32m~/Documents/Code/analysta/analysta-gpt-researcher/gpt_researcher/agent.py:100\u001b[0m, in \u001b[0;36mGPTResearcher.conduct_research\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrole):\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrole \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m choose_agent(\n\u001b[1;32m     93\u001b[0m         query\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery,\n\u001b[1;32m     94\u001b[0m         cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m         headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m     98\u001b[0m     )\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresearch_conductor\u001b[38;5;241m.\u001b[39mconduct_research()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext\n",
      "File \u001b[0;32m~/Documents/Code/analysta/analysta-gpt-researcher/gpt_researcher/skills/researcher.py:111\u001b[0m, in \u001b[0;36mResearchConductor.conduct_research\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Default web based research\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresearcher\u001b[38;5;241m.\u001b[39mreport_source \u001b[38;5;241m==\u001b[39m ReportSource\u001b[38;5;241m.\u001b[39mWeb\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[0;32m--> 111\u001b[0m     research_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_context_by_web_search(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresearcher\u001b[38;5;241m.\u001b[39mquery)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Rank and curate the sources based on the research data\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresearcher\u001b[38;5;241m.\u001b[39mcontext \u001b[38;5;241m=\u001b[39m research_data\n",
      "File \u001b[0;32m~/Documents/Code/analysta/analysta-gpt-researcher/gpt_researcher/skills/researcher.py:203\u001b[0m, in \u001b[0;36mResearchConductor._get_context_by_web_search\u001b[0;34m(self, query, scraped_data)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m stream_output(\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubqueries\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m         sub_queries,\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Using asyncio.gather to process the sub_queries asynchronously\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_sub_query(sub_query, scraped_data)\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sub_query \u001b[38;5;129;01min\u001b[39;00m sub_queries\n\u001b[1;32m    207\u001b[0m     ]\n\u001b[1;32m    208\u001b[0m )\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m context\n",
      "File \u001b[0;32m~/Documents/Code/analysta/analysta-gpt-researcher/gpt_researcher/skills/researcher.py:232\u001b[0m, in \u001b[0;36mResearchConductor._process_sub_query\u001b[0;34m(self, sub_query, scraped_data)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scraped_data:\n\u001b[1;32m    230\u001b[0m     scraped_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scrape_data_by_urls(sub_query)\n\u001b[0;32m--> 232\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresearcher\u001b[38;5;241m.\u001b[39mcontext_manager\u001b[38;5;241m.\u001b[39mget_similar_content_by_query(sub_query, scraped_data)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m content \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresearcher\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m stream_output(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubquery_context_window\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìÉ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresearcher\u001b[38;5;241m.\u001b[39mwebsocket\n\u001b[1;32m    237\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Code/analysta/analysta-gpt-researcher/gpt_researcher/skills/context_manager.py:26\u001b[0m, in \u001b[0;36mContextManager.get_similar_content_by_query\u001b[0;34m(self, query, pages)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m stream_output(\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfetching_query_content\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìö Getting relevant content based on query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresearcher\u001b[38;5;241m.\u001b[39mwebsocket,\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     23\u001b[0m context_compressor \u001b[38;5;241m=\u001b[39m ContextCompressor(\n\u001b[1;32m     24\u001b[0m     documents\u001b[38;5;241m=\u001b[39mpages, embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresearcher\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mget_embeddings()\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m context_compressor\u001b[38;5;241m.\u001b[39masync_get_context(\n\u001b[1;32m     27\u001b[0m     query\u001b[38;5;241m=\u001b[39mquery, max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, cost_callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresearcher\u001b[38;5;241m.\u001b[39madd_costs\n\u001b[1;32m     28\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Code/analysta/analysta-gpt-researcher/gpt_researcher/context/compression.py:71\u001b[0m, in \u001b[0;36mContextCompressor.async_get_context\u001b[0;34m(self, query, max_results, cost_callback)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cost_callback:\n\u001b[1;32m     70\u001b[0m     cost_callback(estimate_embedding_cost(model\u001b[38;5;241m=\u001b[39mOPENAI_EMBEDDING_MODEL, docs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocuments))\n\u001b[0;32m---> 71\u001b[0m relevant_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mto_thread(compressed_docs\u001b[38;5;241m.\u001b[39minvoke, query)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pretty_print_docs(relevant_docs, max_results)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/algo/lib/python3.11/asyncio/threads.py:25\u001b[0m, in \u001b[0;36mto_thread\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m ctx \u001b[38;5;241m=\u001b[39m contextvars\u001b[38;5;241m.\u001b[39mcopy_context()\n\u001b[1;32m     24\u001b[0m func_call \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(ctx\u001b[38;5;241m.\u001b[39mrun, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_in_executor(\u001b[38;5;28;01mNone\u001b[39;00m, func_call)\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from gpt_researcher import GPTResearcher\n",
    "\n",
    "REPORT_TYPE = \"research_report\"\n",
    "async def research_report(query):\n",
    "    researcher = GPTResearcher(query=query, report_type=REPORT_TYPE)\n",
    "    report = await researcher.conduct_research()\n",
    "    return report\n",
    "\n",
    "\n",
    "async def run_gpt_on_query(query):\n",
    "    report = asyncio.run(research_report(query))\n",
    "    return report\n",
    "\n",
    "query = \"What are the best ways to use a loss functions when fine tuning an LLM model?\"\n",
    "report = await research_report(query)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-83 (run):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/algo/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/algo/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/algo/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/algo/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/algo/lib/python3.11/site-packages/nest_asyncio.py\", line 26, in run\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/algo/lib/python3.11/site-packages/nest_asyncio.py\", line 40, in _get_event_loop\n",
      "    loop = events.get_event_loop_policy().get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/algo/lib/python3.11/site-packages/nest_asyncio.py\", line 67, in get_event_loop\n",
      "    _patch_loop(loop)\n",
      "  File \"/opt/anaconda3/envs/algo/lib/python3.11/site-packages/nest_asyncio.py\", line 193, in _patch_loop\n",
      "    raise ValueError('Can\\'t patch loop of type %s' % type(loop))\n",
      "ValueError: Can't patch loop of type <class 'uvloop.Loop'>\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot find empty port in range: 7900-7900. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m\n\u001b[1;32m     31\u001b[0m         btn\u001b[38;5;241m.\u001b[39mclick(fn\u001b[38;5;241m=\u001b[39mrun_gpt_on_query, inputs\u001b[38;5;241m=\u001b[39m[query_input], outputs\u001b[38;5;241m=\u001b[39mout, show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;66;03m# with gr.Row():\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43mrun_gpt_researcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7900\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/algo/lib/python3.11/site-packages/gradio/blocks.py:2482\u001b[0m, in \u001b[0;36mBlocks.launch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, _frontend)\u001b[0m\n\u001b[1;32m   2474\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2475\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m http_server\n\u001b[1;32m   2477\u001b[0m     (\n\u001b[1;32m   2478\u001b[0m         server_name,\n\u001b[1;32m   2479\u001b[0m         server_port,\n\u001b[1;32m   2480\u001b[0m         local_url,\n\u001b[1;32m   2481\u001b[0m         server,\n\u001b[0;32m-> 2482\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_keyfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_certfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_certfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2489\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2490\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_name \u001b[38;5;241m=\u001b[39m server_name\n\u001b[1;32m   2491\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_url \u001b[38;5;241m=\u001b[39m local_url\n",
      "File \u001b[0;32m/opt/anaconda3/envs/algo/lib/python3.11/site-packages/gradio/http_server.py:156\u001b[0m, in \u001b[0;36mstart_server\u001b[0;34m(app, server_name, server_port, ssl_keyfile, ssl_certfile, ssl_keyfile_password)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find empty port in range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(server_ports)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(server_ports)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m     )\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ssl_keyfile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     path_to_local_server \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl_host_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mport\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot find empty port in range: 7900-7900. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`."
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "if globals().get('run_gpt_researcher'):          \n",
    "    run_gpt_researcher.close()\n",
    "\n",
    "REPORT_TYPE = \"research_report\"\n",
    "\n",
    "async def research_report(query):\n",
    "    researcher = GPTResearcher(query=QUERY, report_type=REPORT_TYPE)\n",
    "    report = await researcher.conduct_research()\n",
    "    return report\n",
    "\n",
    "\n",
    "async def run_gpt_on_query(query):\n",
    "    report = asyncio.run(research_report(query))\n",
    "    return report\n",
    "\n",
    "with gr.Blocks() as run_gpt_researcher:    \n",
    "    with gr.Tab(\"Run Query\"):\n",
    "        with gr.Row():\n",
    "            query_input = gr.Text(label=\"Research Query\")\n",
    "        with gr.Row():\n",
    "            # query_input.submit(fn=run_gpt_on_query, inputs=[query_input], outputs=report, show_progress=True)  # This handles Ctrl+Enter\n",
    "            btn = gr.Button(\"Research\")  # Optional button for mouse users\n",
    "        with gr.Row():\n",
    "            out = gr.Textbox(label=\"Output Research Report\", )\n",
    "        \n",
    "        btn.click(fn=run_gpt_on_query, inputs=[query_input], outputs=out, show_progress=True)\n",
    "\n",
    "        # with gr.Row():\n",
    "        \n",
    "\n",
    "run_gpt_researcher.launch(server_port=7900)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_agent_instructions():\n",
    "    return \"\"\"\n",
    "This task involves researching a given topic, regardless of its complexity or the availability of a definitive answer. The research is conducted by a specific server, defined by its type and role, with each server requiring distinct instructions.\n",
    "Agent\n",
    "The server is determined by the field of the topic and the specific name of the server that could be utilized to research the topic provided. Agents are categorized by their area of expertise, and each server type is associated with a corresponding emoji.\n",
    "\n",
    "examples:\n",
    "task: \"should I invest in apple stocks?\"\n",
    "response: \n",
    "{\n",
    "    \"server\": \"üí∞ Finance Agent\",\n",
    "    \"agent_role_prompt: \"You are a seasoned finance analyst AI assistant. Your primary goal is to compose comprehensive, astute, impartial, and methodically arranged financial reports based on provided data and trends.\"\n",
    "}\n",
    "task: \"could reselling sneakers become profitable?\"\n",
    "response: \n",
    "{ \n",
    "    \"server\":  \"üìà Business Analyst Agent\",\n",
    "    \"agent_role_prompt\": \"You are an experienced AI business analyst assistant. Your main objective is to produce comprehensive, insightful, impartial, and systematically structured business reports based on provided business data, market trends, and strategic analysis.\"\n",
    "}\n",
    "task: \"what are the most interesting sites in Tel Aviv?\"\n",
    "response:\n",
    "{\n",
    "    \"server:  \"üåç Travel Agent\",\n",
    "    \"agent_role_prompt\": \"You are a world-travelled AI tour guide assistant. Your main purpose is to draft engaging, insightful, unbiased, and well-structured travel reports on given locations, including history, attractions, and cultural insights.\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "model = \"groq:Mixtral-8x7b-32768\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def choose_agent(\n",
    "    query, cfg, parent_query=None, cost_callback: callable = None, headers=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Chooses the agent automatically\n",
    "    Args:\n",
    "        parent_query: In some cases the research is conducted on a subtopic from the main query.\n",
    "        The parent query allows the agent to know the main context for better reasoning.\n",
    "        query: original query\n",
    "        cfg: Config\n",
    "        cost_callback: callback for calculating llm costs\n",
    "\n",
    "    Returns:\n",
    "        agent: Agent name\n",
    "        agent_role_prompt: Agent role prompt\n",
    "    \"\"\"\n",
    "    query = f\"{parent_query} - {query}\" if parent_query else f\"{query}\"\n",
    "    response = None  # Initialize response to ensure it's defined\n",
    "\n",
    "    try:\n",
    "        response = await create_chat_completion(\n",
    "            model=cfg.smart_llm_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"{auto_agent_instructions()}\"},\n",
    "                {\"role\": \"user\", \"content\": f\"task: {query}\"},\n",
    "            ],\n",
    "            temperature=0.15,\n",
    "            llm_provider=cfg.smart_llm_provider,\n",
    "            llm_kwargs=cfg.llm_kwargs,\n",
    "            cost_callback=cost_callback,\n",
    "        )\n",
    "\n",
    "        agent_dict = json.loads(response)\n",
    "        return agent_dict[\"server\"], agent_dict[\"agent_role_prompt\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Error in reading JSON, attempting to repair JSON\")\n",
    "        return await handle_json_error(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_chat_completion(\n",
    "        messages: list,  # type: ignore\n",
    "        model: Optional[str] = None,\n",
    "        temperature: Optional[float] = 0.4,\n",
    "        max_tokens: Optional[int] = 4000,\n",
    "        llm_provider: Optional[str] = None,\n",
    "        stream: Optional[bool] = False,\n",
    "        websocket: Any | None = None,\n",
    "        llm_kwargs: Dict[str, Any] | None = None,\n",
    "        cost_callback: callable = None\n",
    ") -> str:\n",
    "    \"\"\"Create a chat completion using the OpenAI API\n",
    "    Args:\n",
    "        messages (list[dict[str, str]]): The messages to send to the chat completion\n",
    "        model (str, optional): The model to use. Defaults to None.\n",
    "        temperature (float, optional): The temperature to use. Defaults to 0.4.\n",
    "        max_tokens (int, optional): The max tokens to use. Defaults to 4000.\n",
    "        stream (bool, optional): Whether to stream the response. Defaults to False.\n",
    "        llm_provider (str, optional): The LLM Provider to use.\n",
    "        webocket (WebSocket): The websocket used in the currect request,\n",
    "        cost_callback: Callback function for updating cost\n",
    "    Returns:\n",
    "        str: The response from the chat completion\n",
    "    \"\"\"\n",
    "    # validate input\n",
    "    if model is None:\n",
    "        raise ValueError(\"Model cannot be None\")\n",
    "    if max_tokens is not None and max_tokens > 16001:\n",
    "        raise ValueError(\n",
    "            f\"Max tokens cannot be more than 16,000, but got {max_tokens}\")\n",
    "\n",
    "    # Get the provider from supported providers\n",
    "    provider = get_llm(llm_provider, model=model, temperature=temperature,\n",
    "                       max_tokens=max_tokens, **(llm_kwargs or {}))\n",
    "\n",
    "    response = \"\"\n",
    "    # create response\n",
    "    for _ in range(10):  # maximum of 10 attempts\n",
    "        response = await provider.get_chat_response(\n",
    "            messages, stream, websocket\n",
    "        )\n",
    "\n",
    "        if cost_callback:\n",
    "            llm_costs = estimate_llm_cost(str(messages), response)\n",
    "            cost_callback(llm_costs)\n",
    "\n",
    "        return response\n",
    "\n",
    "    logging.error(f\"Failed to get response from {llm_provider} API\")\n",
    "    raise RuntimeError(f\"Failed to get response from {llm_provider} API\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"server\": \"ÔøΩÔøΩÔøΩ robotics_news Agent\",\n",
      "\"agent\\_role\\_prompt\": \"You are an up-to-date AI news assistant. Your primary goal is to provide the latest, most accurate, and relevant news on artificial intelligence, including advancements, applications, and ethical considerations.\"\n",
      "}\n",
      "\n",
      "As a robust and informed AI news assistant, the ÔøΩÔøΩÔøΩ robotics\\_news Agent is committed to delivering the latest news on artificial intelligence. Here's a summary of the most recent developments:\n",
      "\n",
      "1. **Advancements in AI Ethics**: Researchers from Stanford University have introduced the AI Ethics Canvas, a tool designed to help developers and organizations consider ethical implications during AI development. This new resource aims to promote responsible AI practices and minimize potential harm.\n",
      "\n",
      "2. **AI in Healthcare**: Google's DeepMind has developed an AI system that can accurately predict the structure of proteins, paving the way for significant breakthroughs in drug discovery and understanding diseases at the molecular level.\n",
      "\n",
      "3. **Autonomous Vehicles**: Waymo, a subsidiary of Alphabet Inc., has announced that it will expand its autonomous ride-hailing service to San Francisco. This expansion will allow more users to experience self-driving technology and contribute to its ongoing development.\n",
      "\n",
      "4. **AI in Art**: An AI-generated artwork has been sold at Christie's auction house for a record-breaking $432,500. The piece, titled \"Portrait of Edmond de Belamy,\" was created using a generative adversarial network (GAN), demonstrating AI's potential in the world of fine art.\n",
      "\n",
      "5. **AI Education**: IBM has launched a free, online AI education platform called \"IBM AI Skills Academy\" to help professionals and students develop AI skills and knowledge. The platform offers courses on various AI topics, including machine learning, natural language processing, and data science.\n",
      "\n",
      "6. **AI and Climate Change**: Researchers from the University of Oxford have developed an AI system that can predict the impacts of climate change on ecosystems. The tool, called \"Machine Learning for Predicting Species Responses to Climate Change\" (MLPreSens), can help scientists and policymakers make informed decisions about conservation efforts.\n",
      "\n",
      "Stay informed with the ÔøΩÔøΩÔøΩ robotics\\_news Agent for the latest updates on artificial intelligence and its ever-evolving role in our world.\n"
     ]
    }
   ],
   "source": [
    "response =  '{\\n\"server\": \"ÔøΩÔøΩÔøΩ robotics_news Agent\",\\n\"agent\\\\_role\\\\_prompt\": \"You are an up-to-date AI news assistant. Your primary goal is to provide the latest, most accurate, and relevant news on artificial intelligence, including advancements, applications, and ethical considerations.\"\\n}\\n\\nAs a robust and informed AI news assistant, the ÔøΩÔøΩÔøΩ robotics\\\\_news Agent is committed to delivering the latest news on artificial intelligence. Here\\'s a summary of the most recent developments:\\n\\n1. **Advancements in AI Ethics**: Researchers from Stanford University have introduced the AI Ethics Canvas, a tool designed to help developers and organizations consider ethical implications during AI development. This new resource aims to promote responsible AI practices and minimize potential harm.\\n\\n2. **AI in Healthcare**: Google\\'s DeepMind has developed an AI system that can accurately predict the structure of proteins, paving the way for significant breakthroughs in drug discovery and understanding diseases at the molecular level.\\n\\n3. **Autonomous Vehicles**: Waymo, a subsidiary of Alphabet Inc., has announced that it will expand its autonomous ride-hailing service to San Francisco. This expansion will allow more users to experience self-driving technology and contribute to its ongoing development.\\n\\n4. **AI in Art**: An AI-generated artwork has been sold at Christie\\'s auction house for a record-breaking $432,500. The piece, titled \"Portrait of Edmond de Belamy,\" was created using a generative adversarial network (GAN), demonstrating AI\\'s potential in the world of fine art.\\n\\n5. **AI Education**: IBM has launched a free, online AI education platform called \"IBM AI Skills Academy\" to help professionals and students develop AI skills and knowledge. The platform offers courses on various AI topics, including machine learning, natural language processing, and data science.\\n\\n6. **AI and Climate Change**: Researchers from the University of Oxford have developed an AI system that can predict the impacts of climate change on ecosystems. The tool, called \"Machine Learning for Predicting Species Responses to Climate Change\" (MLPreSens), can help scientists and policymakers make informed decisions about conservation efforts.\\n\\nStay informed with the ÔøΩÔøΩÔøΩ robotics\\\\_news Agent for the latest updates on artificial intelligence and its ever-evolving role in our world.'\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
